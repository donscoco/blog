<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2018%2F11%2F28%2Fgit%2F</url>
    <content type="text"><![CDATA[title:git的学习笔记date:2018-1-30 categories:util tags:git前言 这里是donscoco的git学习笔记，发到网上便于笔者复习回顾，原文来自以下网址https://git-scm.com/http://git.oschina.net/progit/http://www.bootcss.com/p/git-guide/https://progit.bootcss.com/ git入门 本章介绍开始使用 Git 前的相关知识。我们会先了解一些版本控制工具的历史背景，然后试着让 Git 在你的系统上跑起来，直到最后配置好，可以正常开始开发工作。读完本章，你就会明白为什么 Git 会如此流行，为什么你应该立即开始使用它。 git 是一款用于代码版本控制代码管理工具 关于版本控制什么是版本控制？版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。这里git说是代码版本控制管理但是我们可以对任何类型的文件进行版本控制。 有了版本控制工具，你就可以将某个文件回溯到之前的状态，甚至将整个项目都回退到过去某个时间点的状态。你可以比较文件的变化细节，查出最后是谁修改了哪个地方，从而找出导致怪异问题出现的原因，又是谁在何时报告了某个功能缺陷等等。使用版本控制系统通常还意味着，就算你乱来一气把整个项目中的文件改的改删的删，你也照样可以轻松恢复到原先的样子。但额外增加的工作量却微乎其微。 本地版本控制系统许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。这么做唯一的好处就是简单。不过坏处也不少：有时候会混淆所在的工作目录，一旦弄错文件丢了数据就没法撤销恢复。为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异 其中最流行的一种叫做 rcs，现今许多计算机系统上都还看得到它的踪影。甚至在流行的 Mac OS X 系统上安装了开发者工具包之后，也可以使用 rcs 命令。它的工作原理基本上就是保存并管理文件补丁（patch）。文件补丁是一种特定格式的文本文件，记录着对应文件修订前后的内容变化。所以，根据每次修订后的补丁，rcs 可以通过不断打补丁，计算出各个版本的文件内容。 集中化的版本控制系统接下来人们又遇到一个问题，如何让在不同系统上的开发者协同工作？于是，集中化的版本控制系统（ Centralized Version Control Systems，简称 CVCS ）应运而生。这类系统，诸如 CVS，Subversion 以及 Perforce 等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。多年以来，这已成为版本控制系统的标准做法 这种做法带来了许多好处，特别是相较于老式的本地 VCS 来说。现在，每个人都可以在一定程度上看到项目中的其他人正在做些什么。而管理员也可以轻松掌控每个开发者的权限，并且管理一个 CVCS 要远比在各个客户端上维护本地数据库来得轻松容易。 事分两面，有好有坏。这么做最显而易见的缺点是中央服务器的单点故障。如果宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。要是中央服务器的磁盘发生故障，碰巧没做备份，或者备份不够及时，就会有丢失数据的风险。最坏的情况是彻底丢失整个项目的所有历史更改记录，而被客户端偶然提取出来的保存在本地的某些快照数据就成了恢复数据的希望。但这样的话依然是个问题，你不能保证所有的数据都已经有人事先完整提取出来过。本地版本控制系统也存在类似问题，只要整个项目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。 分布式版本控制系统于是分布式版本控制系统（ Distributed Version Control System，简称 DVCS ）面世了。在这类系统中，像 Git，Mercurial，Bazaar 以及 Darcs 等，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。因为每一次的提取操作，实际上都是一次对代码仓库的完整备份 更进一步，许多这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。你可以根据需要设定不同的协作流程，比如层次模型式的工作流，而这在以前的集中式系统中是无法实现的。 Git 简史 同生活中的许多伟大事件一样，Git 诞生于一个极富纷争大举创新的年代。Linux 内核开源项目有着为数众广的参与者。绝大多数的 Linux 内核维护工作都花在了提交补丁和保存归档的繁琐事务上（1991－2002年间）。到 2002 年，整个项目组开始启用分布式版本控制系统 BitKeeper 来管理和维护代码。 到了 2005 年，开发 BitKeeper 的商业公司同 Linux 内核开源社区的合作关系结束，他们收回了免费使用 BitKeeper 的权力。这就迫使 Linux 开源社区（特别是 Linux 的缔造者 Linus Torvalds ）不得不吸取教训，只有开发一套属于自己的版本控制系统才不至于重蹈覆辙。他们对新的系统制订了若干目标： 速度 简单的设计 对非线性开发模式的强力支持 完全分布式 有能力高效管理类似linux内核一样的超大规模项目（速度和数据量） 自诞生于 2005 年以来，Git 日臻成熟完善，在高度易用的同时，仍然保留着初期设定的目标。它的速度飞快，极其适合管理大项目，它还有着令人难以置信的非线性分支管理系统（见第三章），可以应付各种复杂的项目开发需求。 Git基础 那么，简单地说，Git 究竟是怎样的一个系统呢？请注意，接下来的内容非常重要，若是理解了 Git 的思想和基本工作原理，用起来就会知其所以然，游刃有余。在开始学习 Git 的时候，请不要尝试把各种概念和其他版本控制系统（诸如 Subversion 和 Perforce 等）相比拟，否则容易混淆每个操作的实际意义。 Git 在保存和处理各种信息的时候，虽然操作起来的命令形式非常相近，但它与其他版本控制系统的做法颇为不同。理解这些差异将有助于你准确地使用 Git 提供的各种工具。 直接记录快照，而非差异比较Git 和其他版本控制系统的主要差别在于，Git 只关心文件数据的整体是否发生变化，而大多数其他系统则只关心文件内容的具体差异。这类系统（CVS，Subversion，Perforce，Bazaar 等等）每次记录有哪些文件作了更新，以及都更新了哪些行的什么内容，如下图Git 并不保存这些前后变化的差异数据。实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一链接。Git 的工作方式就像下图 这是 Git 同其他系统的重要区别。它完全颠覆了传统版本控制的套路，并对各个环节的实现方式作了新的设计。Git 更像是个小型的文件系统，但它同时还提供了许多以此为基础的超强工具，而不只是一个简单的 VCS。稍后在第三章讨论 Git 分支管理的时候，我们会再看看这样的设计究竟会带来哪些好处。 近乎所有操作都是本地执行在 Git 中的绝大多数操作都只需要访问本地文件和资源，不用连网。但如果用 CVCS 的话，差不多所有操作都需要连接网络。因为 Git 在本地磁盘上就保存着所有当前项目的历史更新，所以处理起来速度飞快。 举个例子，如果要浏览项目的历史更新摘要，Git 不用跑到外面的服务器上去取数据回来，而直接从本地数据库读取后展示给你看。所以任何时候你都可以马上翻阅，无需等待。如果想要看当前版本的文件和一个月前的版本之间有何差异，Git 会取出一个月前的快照和当前文件作一次差异运算，而不用请求远程服务器来做这件事，或是把老版本的文件拉到本地来作比较。 用 CVCS 的话，没有网络或者断开 VPN 你就无法做任何事情。但用 Git 的话，就算你在飞机或者火车上，都可以非常愉快地频繁提交更新，等到了有网络的时候再上传到远程仓库。同样，在回家的路上，不用连接 VPN 你也可以继续工作。换作其他版本控制系统，这么做几乎不可能，抑或非常麻烦。比如 Perforce，如果不连到服务器，几乎什么都做不了（译注：默认无法发出命令 p4 edit file 开始编辑文件，因为 Perforce 需要联网通知系统声明该文件正在被谁修订。但实际上手工修改文件权限可以绕过这个限制，只是完成后还是无法提交更新。）；如果是 Subversion 或 CVS，虽然可以编辑文件，但无法提交更新，因为数据库在网络上。看上去好像这些都不是什么大问题，但实际体验过之后，你就会惊喜地发现，这其实是会带来很大不同的。 时刻保持数据完整性在保存到 Git 之前，所有数据都要进行内容的校验和（checksum）计算，并将此结果作为数据的唯一标识和索引。换句话说，不可能在你修改了文件或目录之后，Git 一无所知。这项特性作为 Git 的设计哲学，建在整体架构的最底层。所以如果文件在传输时变得不完整，或者磁盘损坏导致文件数据缺失，Git 都能立即察觉。 Git 使用 SHA-1 算法计算数据的校验和，通过对文件的内容或目录的结构计算出一个 SHA-1 哈希值，作为指纹字符串。该字串由 40 个十六进制字符（0-9 及 a-f）组成，看起来就像是：24b9da6552252987aa493b52f8696cd6d3b00373Git 的工作完全依赖于这类指纹字串，所以你会经常看到这样的哈希值。实际上，所有保存在 Git 数据库中的东西都是用此哈希值来作索引的，而不是靠文件名。 多数操作仅添加数据常用的 Git 操作大多仅仅是把数据添加到数据库。因为任何一种不可逆的操作，比如删除数据，都会使回退或重现历史版本变得困难重重。在别的 VCS 中，若还未提交更新，就有可能丢失或者混淆一些修改的内容，但在 Git 里，一旦提交快照之后就完全不用担心丢失数据，特别是养成定期推送到其他仓库的习惯的话。 这种高可靠性令我们的开发工作安心不少，尽管去做各种试验性的尝试好了，再怎样也不会弄丢数据。至于 Git 内部究竟是如何保存和恢复数据的，我们会在第九章讨论 Git 内部原理时再作详述。 文件的三种状态好，现在请注意，接下来要讲的概念非常重要。对于任何一个文件，在 Git 内都只有三种状态：已提交（committed），已修改（modified）和已暂存（staged）。已提交表示该文件已经被安全地保存在本地数据库中了；已修改表示修改了某个文件，但还没有提交保存；已暂存表示把已修改的文件放在下次提交时要保存的清单中。 每个项目都有一个 Git 目录（译注：如果 git clone 出来的话，就是其中 .git 的目录；如果 git clone –bare 的话，新建的目录本身就是 Git 目录。），它是 Git 用来保存元数据和对象数据库的地方。该目录非常重要，每次克隆镜像仓库的时候，实际拷贝的就是这个目录里面的数据。 从项目中取出某个版本的所有文件和目录，用以开始后续工作的叫做工作目录。这些文件实际上都是从 Git 目录中的压缩对象数据库中提取出来的，接下来就可以在工作目录中对这些文件进行编辑。 所谓的暂存区域只不过是个简单的文件，一般都放在 Git 目录中。有时候人们会把这个文件叫做索引文件，不过标准说法还是叫暂存区域。 基本的 Git 工作流程如下： 在工作目录中修改某些文件。 对修改后的文件进行快照，然后保存到暂存区域。 提交更新，将保存在暂存区域的文件快照永久转储到 Git 目录中。 所以，我们可以从文件所处的位置来判断状态：如果是 Git 目录中保存着的特定版本文件，就属于已提交状态；如果作了修改并已放入暂存区域，就属于已暂存状态；如果自上次取出后，作了修改但还没有放到暂存区域，就是已修改状态。到第二章的时候，我们会进一步了解其中细节，并学会如何根据文件状态实施后续操作，以及怎样跳过暂存直接提交。 安装 Git 是时候动手尝试下 Git 了，不过得先安装好它。有许多种安装方式，主要分为两种，一种是通过编译源代码来安装；另一种是使用为特定平台预编译好的安装包。 从源代码安装若是条件允许，从源代码安装有很多好处，至少可以安装最新的版本。Git 的每个版本都在不断尝试改进用户体验，所以能通过源代码自己编译安装最新版本就再好不过了。有些 Linux 版本自带的安装包更新起来并不及时，所以除非你在用最新的 distro 或者 backports，那么从源代码安装其实该算是最佳选择。 Git 的工作需要调用 curl，zlib，openssl，expat，libiconv 等库的代码，所以需要先安装这些依赖工具。在有 yum 的系统上（比如 Fedora）或者有 apt-get 的系统上（比如 Debian 体系），可以用下面的命令安装： 1[root@iZzf6hdnuqzu7wZ ~]# yum install curl-devel zlib-devel openssl-devel expat-devel gettext-devel 1$ apt-get install libcurl4-gnutls-dev libexpat1-dev gettext libz-dev libssl-dev 之后，从下面的 Git 官方站点下载最新版本源代码：http://git-scm.com/download,以下仅仅作为演示，可以到官网查看最新版本和文档 1[root@iZzf6hdnuqzu7wZ ~]# wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.10.1.tar.gz 然后解压，编译并安装： 1234$ tar -zxvf git-2.10.1.tar.gz$ cd git-2.10.1$ make prefix=/usr/local all$ sudo make prefix=/usr/local install 现在已经可以用 git 命令了，用 git 把 Git 项目仓库克隆到本地，以便日后随时更新： 1[root@iZzf6hdnuqzu7wZ gitdir]# /usr/local/git/bin/git clone git@github.com:donscoco/donscoco.github.io.git 在 Linux 上安装如果要在 Linux 上安装预编译好的 Git 二进制安装包，可以直接用系统提供的包管理工具。用 yum 安装： 1[root@VM_0_8_centos ~]# yum install git-core 在 Ubuntu 这类 Debian 体系的系统上，可以用 apt-get 安装： 1$ apt-get install git 在 Mac 上安装在 Mac 上安装 Git 有两种方式。最容易的当属使用图形化的 Git 安装工具，界面如图，下载地址在：http://code.google.com/p/git-osx-installer另一种是通过 MacPorts (http://www.macports.org) 安装。如果已经装好了 MacPorts，用下面的命令安装 Git：1$ sudo port install git-core +svn +doc +bash_completion +gitweb 这种方式就不需要再自己安装依赖库了，Macports 会帮你搞定这些麻烦事。一般上面列出的安装选项已经够用，要是你想用 Git 连接 Subversion 的代码仓库，还可以加上 +svn 选项，具体将在第八章作介绍。（译注：还有一种是使用 homebrew（https://github.com/mxcl/homebrew）：brew install git。） 在 Windows 上安装在 Windows 上安装 Git 同样轻松，有个叫做 msysGit 的项目提供了安装包，可以到 GitHub 的页面上下载 exe 安装文件并运行：http://msysgit.github.com/完成安装之后，就可以使用命令行的 git 工具（已经自带了 ssh 客户端）了，另外还有一个图形界面的 Git 项目管理工具。 初次运行 Git 前的配置 一般在新的系统上，我们都需要先配置下自己的 Git 工作环境。配置工作只需一次，以后升级时还会沿用现在的配置。当然，如果需要，你随时可以用相同的命令修改已有的配置。 Git 提供了一个叫做 git config 的工具（译注：实际是 git-config 命令，只不过可以通过 git 加一个名字来呼叫此命令。），专门用来配置或读取相应的工作环境变量。而正是由这些环境变量，决定了 Git 在各个环节的具体工作方式和行为。这些变量可以存放在以下三个不同的地方： /etc/gitconfig 文件：系统中对所有用户都普遍适用的配置。若使用 git config 时用 –system 选项，读写的就是这个文件。 ~/.gitconfig 文件：用户目录下的配置文件只适用于该用户。若使用 git config 时用 –global 选项，读写的就是这个文件。 当前项目的 git 目录中的配置文件（也就是工作目录中的 .git/config 文件）：这里的配置仅仅针对当前项目有效。每一个级别的配置都会覆盖上层的相同配置，所以 .git/config 里的配置会覆盖 /etc/gitconfig 中的同名变量。 在 Windows 系统上，Git 会找寻用户主目录下的 .gitconfig 文件。主目录即 $HOME 变量指定的目录，一般都是 C:\Users\$USER。此外，Git 还会尝试找寻 /etc/gitconfig 文件，只不过看当初 Git 装在什么目录，就以此作为根目录来定位。 用户信息第一个要配置的是你个人的用户名称和电子邮件地址。这两条配置很重要，每次 Git 提交时都会引用这两条信息，说明是谁提交了更新，所以会随更新内容一起被永久纳入历史记录： 12[root@VM_0_8_centos ~]# git config --global user.name "donscoco"[root@VM_0_8_centos ~]# git config --global user.emal donscoco@gmail.com 如果用了 –global 选项，那么更改的配置文件就是位于你用户主目录下的那个，以后你所有的项目都会默认使用这里配置的用户信息。如果要在某个特定的项目中使用其他名字或者电邮，只要去掉 –global 选项重新配置即可，新的设定保存在当前项目的 .git/config 文件里。 文本编辑器接下来要设置的是默认使用的文本编辑器。Git 需要你输入一些额外消息的时候，会自动调用一个外部文本编辑器给你用。默认会使用操作系统指定的默认编辑器，一般可能会是 Vi 或者 Vim。如果你有其他偏好，比如 Emacs 的话，可以重新设置： 1$ git config --global core.editor emacs 差异分析工具还有一个比较常用的是，在解决合并冲突时使用哪种差异分析工具。比如要改用 vimdiff 的话：1$ git config --global merge.tool vimdiff Git 可以理解 kdiff3，tkdiff，meld，xxdiff，emerge，vimdiff，gvimdiff，ecmerge，和 opendiff 等合并工具的输出信息。当然，你也可以指定使用自己开发的工具，具体怎么做可以参阅第七章 查看配置信息要检查已有的配置信息，可以使用 git config –list 命令：123[root@VM_0_8_centos vim74]# git config --listuser.name=donscocouser.emal=286563721@qq.com 有时候会看到重复的变量名，那就说明它们来自不同的配置文件（比如 /etc/gitconfig 和 ~/.gitconfig），不过最终 Git 实际采用的是最后一个。 也可以直接查阅某个环境变量的设定，只要把特定的名字跟在后面即可，像这样： 12[root@VM_0_8_centos vim74]# git config user.namedonscoco 获取帮助 想了解 Git 的各式工具该怎么用，可以阅读它们的使用帮助，方法有三： $ git help $ git –help $ man git- 例如123456789101112131415161718192021[root@VM_0_8_centos vim74]# git help configGIT-CONFIG(1) Git Manual GIT-CONFIG(1)NAME git-config - Get and set repository or global optionsSYNOPSIS git config [&lt;file-option&gt;] [type] [-z|--null] name [value [value_regex]] git config [&lt;file-option&gt;] [type] --add name value git config [&lt;file-option&gt;] [type] --replace-all name value [value_regex] git config [&lt;file-option&gt;] [type] [-z|--null] --get name [value_regex] git config [&lt;file-option&gt;] [type] [-z|--null] --get-all name [value_regex] git config [&lt;file-option&gt;] [type] [-z|--null] --get-regexp name_regex [value_regex] git config [&lt;file-option&gt;] --unset name [value_regex] git config [&lt;file-option&gt;] --unset-all name [value_regex] git config [&lt;file-option&gt;] --rename-section old_name new_name git config [&lt;file-option&gt;] --remove-section name git config [&lt;file-option&gt;] [-z|--null] -l | --list git config [&lt;file-option&gt;] --get-color name [default] git config [&lt;file-option&gt;] --get-colorbool name [stdout-is-tty] git config [&lt;file-option&gt;] -e | --edit git基础读完本章你就能上手使用 Git 了。本章将介绍几个最基本的，也是最常用的 Git 命令，以后绝大多数时间里用到的也就是这几个命令。读完本章，你就能初始化一个新的代码仓库，做一些适当配置；开始或停止跟踪某些文件；暂存或提交某些更新。我们还会展示如何让 Git 忽略某些文件，或是名称符合特定模式的文件；如何既快且容易地撤消犯下的小错误；如何浏览项目的更新历史，查看某两次更新之间的差异；以及如何从远程仓库拉数据下来或者推数据上去。 取得项目的 Git 仓库 有两种取得 Git 项目仓库的方法。 第一种是在现存的目录下，通过导入所有文件来创建新的 Git 仓库。 第二种是从已有的 Git 仓库克隆出一个新的镜像仓库来。 在工作目录中初始化新仓库要对现有的某个项目开始用 Git 管理，只需到此项目所在的目录，执行：1[root@VM_0_8_centos init_dir]# git init 初始化后，在当前目录下会出现一个名为 .git 的目录，所有 Git 需要的数据和资源都存放在这个目录中。不过目前，仅仅是按照既有的结构框架初始化好了里边所有的文件和目录，但我们还没有开始跟踪管理项目中的任何一个文件。（在第九章我们会详细说明刚才创建的 .git 目录中究竟有哪些文件，以及都起些什么作用。） 如果当前目录下有几个文件想要纳入版本控制，需要先用 git add 命令告诉 Git 开始对这些文件进行跟踪，然后提交： 123456[root@VM_0_8_centos init_dir]# git add README[root@VM_0_8_centos init_dir]# git commit -m 'init file'[master (root-commit) 4031010] init file 1 files changed, 1 insertions(+), 0 deletions(-) create mode 100644 README[root@VM_0_8_centos init_dir]# 稍后我们再逐一解释每条命令的意思。不过现在，你已经得到了一个实际维护着若干文件的 Git 仓库。 从现有仓库克隆如果想对某个开源项目出一份力，可以先把该项目的 Git 仓库复制一份出来，这就需要用到 git clone 命令。如果你熟悉其他的 VCS 比如 Subversion，你可能已经注意到这里使用的是 clone 而不是 checkout。这是个非常重要的差别，Git 收取的是项目历史的所有数据（每一个文件的每一个版本），服务器上有的数据克隆之后本地也都有了。实际上，即便服务器的磁盘发生故障，用任何一个克隆出来的客户端都可以重建服务器上的仓库，回到当初克隆时的状态（虽然可能会丢失某些服务器端的挂钩设置，但所有版本的数据仍旧还在，有关细节请参考第四章）。 克隆仓库的命令格式为 git clone [url]。比如，要克隆 Ruby 语言的 Git 代码仓库 Grit，可以用下面的命令：123456[root@VM_0_8_centos init_dir]# git clone git://github.com/schacon/grit.gitInitialized empty Git repository in /home/dons/git_learning/init_dir/grit/.git/remote: Enumerating objects: 4051, done.remote: Total 4051 (delta 0), reused 0 (delta 0), pack-reused 4051Receiving objects: 100% (4051/4051), 2.04 MiB | 733 KiB/s, done.Resolving deltas: 100% (1465/1465), done. 这会在当前目录下创建一个名为grit的目录，其中包含一个 .git 的目录，用于保存下载下来的所有版本记录，然后从中取出最新版本的文件拷贝。如果进入这个新建的 grit 目录，你会看到项目中的所有文件已经在里边了，准备好后续的开发和使用。如果希望在克隆的时候，自己定义要新建的项目目录名称，可以在上面的命令末尾指定新的名字：123456[root@VM_0_8_centos init_dir]# git clone git://github.com/schacon/grit.git mygritInitialized empty Git repository in /home/dons/git_learning/init_dir/mygrit/.git/remote: Enumerating objects: 4051, done.remote: Total 4051 (delta 0), reused 0 (delta 0), pack-reused 4051Receiving objects: 100% (4051/4051), 2.04 MiB | 253 KiB/s, done.Resolving deltas: 100% (1465/1465), done. 唯一的差别就是，现在新建的目录成了 mygrit，其他的都和上边的一样。 Git 支持许多数据传输协议。之前的例子使用的是 git:// 协议，不过你也可以用 http(s):// 或者 user@server:/path.git 表示的 SSH 传输协议。我们会在第四章详细介绍所有这些协议在服务器端该如何配置使用，以及各种方式之间的利弊。 记录每次更新到仓库 现在我们手上已经有了一个真实项目的 Git 仓库，并从这个仓库中取出了所有文件的工作拷贝。接下来，对这些文件作些修改，在完成了一个阶段的目标之后，提交本次更新到仓库。 请记住，工作目录下面的所有文件都不外乎这两种状态：已跟踪或未跟踪。已跟踪的文件是指本来就被纳入版本控制管理的文件，在上次快照中有它们的记录，工作一段时间后，它们的状态可能是未更新，已修改或者已放入暂存区。而所有其他文件都属于未跟踪文件。它们既没有上次更新时的快照，也不在当前的暂存区域。初次克隆某个仓库时，工作目录中的所有文件都属于已跟踪文件，且状态为未修改。 在编辑过某些文件之后，Git 将这些文件标为已修改。我们逐步把这些修改过的文件放到暂存区域，直到最后一次性提交所有这些暂存起来的文件，如此重复。所以使用 Git 时的文件状态变化周期如图 所示。 检查当前文件状态要确定哪些文件当前处于什么状态，可以用 git status 命令。如果在克隆仓库之后立即执行此命令，会看到类似这样的输出：123[root@VM_0_8_centos mygrit]# git status# On branch masternothing to commit (working directory clean) 这说明你现在的工作目录相当干净。换句话说，所有已跟踪文件在上次提交后都未被更改过。此外，上面的信息还表明，当前目录下没有出现任何处于未跟踪的新文件，否则 Git 会在这里列出来。最后，该命令还显示了当前所在的分支是 master，这是默认的分支名称，实际是可以修改的，现在先不用考虑。下一章我们就会详细讨论分支和引用。 现在让我们用 vim 创建一个新文件 README，保存退出后运行 git status 会看到该文件出现在未跟踪文件列表中： 123456789[root@VM_0_8_centos mygrit]# cat READMEthis is mygrit init file[root@VM_0_8_centos mygrit]# git status# On branch master# Untracked files:# (use "git add &lt;file&gt;..." to include in what will be committed)## READMEnothing added to commit but untracked files present (use "git add" to track) 在状态报告中可以看到新建的README文件出现在“Untracked files”下面。未跟踪的文件意味着Git在之前的快照（提交）中没有这些文件；Git 不会自动将之纳入跟踪范围，除非你明明白白地告诉它“我需要跟踪该文件”，因而不用担心把临时文件什么的也归入版本管理。不过现在的例子中，我们确实想要跟踪管理 README 这个文件。 跟踪新文件使用命令 git add 开始跟踪一个新文件。所以，要跟踪 README 文件，运行：12345678[root@VM_0_8_centos mygrit]# git add README[root@VM_0_8_centos mygrit]# git status# On branch master# Changes to be committed:# (use "git reset HEAD &lt;file&gt;..." to unstage)## new file: README# 只要在 “Changes to be committed” 这行下面的，就说明是已暂存状态。如果此时提交，那么该文件此时此刻的版本将被留存在历史记录中。你可能会想起之前我们使用 git init 后就运行了 git add 命令，开始跟踪当前目录下的文件。在 git add 后面可以指明要跟踪的文件或目录路径。如果是目录的话，就说明要递归跟踪该目录下的所有文件。（译注：其实 git add 的潜台词就是把目标文件快照放入暂存区域，也就是 add file into staged area，同时未曾跟踪过的文件标记为需要跟踪。这样就好理解后续 add 操作的实际意义了。） 暂存已修改文件现在我们修改下之前已跟踪过的文件 benchmarks.rb，然后再次运行 status 命令，会看到这样的状态报告：12345678910111213[root@VM_0_8_centos mygrit]# git status# On branch master# Changes to be committed:# (use "git reset HEAD &lt;file&gt;..." to unstage)## new file: README## Changed but not updated:# (use "git add &lt;file&gt;..." to update what will be committed)# (use "git checkout -- &lt;file&gt;..." to discard changes in working directory)## modified: benchmarks.rb# 文件 benchmarks.rb 出现在 “Changes not staged for commit” 这行下面，说明已跟踪文件的内容发生了变化，但还没有放到暂存区。要暂存这次更新，需要运行 git add 命令（这是个多功能命令，根据目标文件的状态不同，此命令的效果也不同：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等）。现在让我们运行 git add 将 benchmarks.rb 放到暂存区，然后再看看 git status 的输出： 123456789[root@VM_0_8_centos mygrit]# git add benchmarks.rb[root@VM_0_8_centos mygrit]# git status# On branch master# Changes to be committed:# (use "git reset HEAD &lt;file&gt;..." to unstage)## new file: README# modified: benchmarks.rb# 现在两个文件都已暂存，下次提交时就会一并记录到仓库。假设此时，你想要在 benchmarks.rb 里再加条注释，重新编辑存盘后，准备好提交。不过且慢，再运行 git status 看看： 1234567891011121314[root@VM_0_8_centos mygrit]# git status# On branch master# Changes to be committed:# (use "git reset HEAD &lt;file&gt;..." to unstage)## new file: README# modified: benchmarks.rb## Changed but not updated:# (use "git add &lt;file&gt;..." to update what will be committed)# (use "git checkout -- &lt;file&gt;..." to discard changes in working directory)## modified: benchmarks.rb# 怎么回事？ benchmarks.rb 文件出现了两次！一次算未暂存，一次算已暂存，这怎么可能呢？好吧，实际上 Git 只不过暂存了你运行 git add 命令时的版本，如果现在提交，那么提交的是添加注释前的版本，而非当前工作目录中的版本。所以，运行了 git add 之后又作了修订的文件，需要重新运行 git add 把最新版本重新暂存起来： 1[root@VM_0_8_centos mygrit]# git add benchmarks.rb 忽略某些文件一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件模式。来看一个实际的例子： 12345[root@VM_0_8_centos mygrit]# cat .gitignore *.[oa]*~pkg.DS_Store 第一行告诉 Git 忽略所有以 .o 或 .a 结尾的文件。一般这类对象文件和存档文件都是编译过程中出现的，我们用不着跟踪它们的版本。第二行告诉 Git 忽略所有以波浪符（~）结尾的文件，许多文本编辑软件（比如 Emacs）都用这样的文件名保存副本。此外，你可能还需要忽略 log，tmp 或者 pid 目录，以及自动生成的文档等等。要养成一开始就设置好 .gitignore 文件的习惯，以免将来误提交这类无用的文件。 文件 .gitignore 的格式规范如下： 所有空行或者以注释符号 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式最后跟反斜杠（/）说明要忽略的是目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。 所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。星号（*）匹配零个或多个任意字符；[abc] 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）；问号（?）只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 我们再看一个 .gitignore 文件的例子： 1234567891011# 此为注释 – 将被 Git 忽略# 忽略所有 .a 结尾的文件*.a# 但 lib.a 除外!lib.a# 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO/TODO# 忽略 build/ 目录下的所有文件build/# 会忽略 doc/notes.txt 但不包括 doc/server/arch.txtdoc/*.txt 查看已暂存和未暂存的更新实际上 git status 的显示比较简单，仅仅是列出了修改过的文件，如果要查看具体修改了什么地方，可以用 git diff 命令。稍后我们会详细介绍 git diff，不过现在，它已经能回答我们的两个问题了：当前做的哪些更新还没有暂存？有哪些更新已经暂存起来准备好了下次提交？ git diff 会使用文件补丁的格式显示具体添加和删除的行。 假如再次修改 README 文件后暂存，然后编辑 benchmarks.rb 文件后先别暂存，运行 status 命令将会看到：123456789101112[root@VM_0_8_centos mygrit]# git diffdiff --git a/benchmarks.rb b/benchmarks.rbindex b7cc2e7..d4a8267 100644--- a/benchmarks.rb+++ b/benchmarks.rb@@ -127,5 +127,5 @@ end main() ##pp Grit::GitRuby.cache_client.stats-##git_learing_test ##git_learning_test_fix +##git_learning_test_3 此命令比较的是工作目录中当前文件和暂存区域快照之间的差异，也就是修改之后还没有暂存起来的变化内容。 若要看已经暂存起来的文件和上次提交时的快照之间的差异，可以用 git diff –cached 命令。（Git 1.6.1 及更高版本还允许使用 git diff –staged，效果是相同的，但更好记些。）来看看实际的效果： 1234567891011121314151617181920[root@VM_0_8_centos mygrit]# git diff --stageddiff --git a/README b/READMEnew file mode 100644index 0000000..6ad2781--- /dev/null+++ b/README@@ -0,0 +1 @@+this is mygrit init filediff --git a/benchmarks.rb b/benchmarks.rbindex e445e28..b7cc2e7 100644--- a/benchmarks.rb+++ b/benchmarks.rb@@ -126,4 +126,6 @@ end main() -##pp Grit::GitRuby.cache_client.stats +##pp Grit::GitRuby.cache_client.stats+##git_learing_test+##git_learning_test_fix 请注意，单单 git diff 不过是显示还没有暂存起来的改动，而不是这次工作和上次提交之间的差异。所以有时候你一下子暂存了所有更新过的文件后，运行 git diff 后却什么也没有，就是这个原因。 像之前说的，暂存 benchmarks.rb 后再编辑，运行 git status 会看到暂存前后的两个版本： 123456789101112131415[root@VM_0_8_centos mygrit]# echo "##git_learning_test_4"&gt;&gt;benchmarks.rb[root@VM_0_8_centos mygrit]# git status# On branch master# Changes to be committed:# (use "git reset HEAD &lt;file&gt;..." to unstage)## new file: README# modified: benchmarks.rb## Changed but not updated:# (use "git add &lt;file&gt;..." to update what will be committed)# (use "git checkout -- &lt;file&gt;..." to discard changes in working directory)## modified: benchmarks.rb# 现在运行 git diff 看暂存前后的变化： 12345678910[root@VM_0_8_centos mygrit]# git diff diff --git a/benchmarks.rb b/benchmarks.rbindex d4a8267..cd27ec1 100644--- a/benchmarks.rb+++ b/benchmarks.rb@@ -129,3 +129,4 @@ main() ##pp Grit::GitRuby.cache_client.stats ##git_learning_test_fix ##git_learning_test_3+##git_learning_test_4 然后用 git diff –cached 查看已经暂存起来的变化： 1234567891011121314151617181920[root@VM_0_8_centos mygrit]# git diff --stageddiff --git a/README b/READMEnew file mode 100644index 0000000..6ad2781--- /dev/null+++ b/README@@ -0,0 +1 @@+this is mygrit init filediff --git a/benchmarks.rb b/benchmarks.rbindex e445e28..d4a8267 100644--- a/benchmarks.rb+++ b/benchmarks.rb@@ -126,4 +126,6 @@ end main() -##pp Grit::GitRuby.cache_client.stats +##pp Grit::GitRuby.cache_client.stats+##git_learning_test_fix +##git_learning_test_3 提交更新现在的暂存区域已经准备妥当可以提交了。在此之前，请一定要确认还有什么修改过的或新建的文件还没有 git add 过，否则提交的时候不会记录这些还没暂存起来的变化。所以，每次准备提交前，先用 git status 看下，是不是都已暂存起来了，然后再运行提交命令 git commit：1234567891011121314# Please enter the commit message for your changes. Lines starting# with '#' will be ignored, and an empty message aborts the commit.## Committer: donscoco &lt;root@VM_0_8_centos.(none)&gt;## On branch master# Changes to be committed:# (use "git reset HEAD &lt;file&gt;..." to unstage)## new file: README# modified: benchmarks.rb#~~ 可以看到，默认的提交消息包含最后一次运行 git status 的输出，放在注释行里，另外开头还有一空行，供你输入提交说明。你完全可以去掉这些注释行，不过留着也没关系，多少能帮你回想起这次更新的内容有哪些。（如果觉得这还不够，可以用 -v 选项将修改差异的每一行都包含到注释中来。）退出编辑器时，Git 会丢掉注释行，将说明内容和本次更新提交到仓库。 另外也可以用 -m 参数后跟提交说明的方式，在一行命令中提交更新： 123$ git commit -m "git first commit"[master ffe1cb8] git first commit 1 file changed, 1 insertion(+) 好，现在你已经创建了第一个提交！可以看到，提交后它会告诉你，当前是在哪个分支（master）提交的，本次提交的完整 SHA-1 校验和是什么（463dc4f），以及在本次提交中，有多少文件修订过，多少行添改和删改过。 记住，提交时记录的是放在暂存区域的快照，任何还未暂存的仍然保持已修改状态，可以在下次提交时纳入版本管理。每一次运行提交操作，都是对你项目作一次快照，以后可以回到这个状态，或者进行比较。 当然，实际上会是这样 ヽ(￣▽￣)ﾉ，这是因为没有配置./ssh .后面会说12345678910111213141516[root@VM_0_8_centos mygrit]# git commit -m "story 182: fix benchmarks for test"[master c103d76] story 182: fix benchmarks for test Committer: donscoco &lt;root@VM_0_8_centos.(none)&gt;Your name and email address were configured automatically basedon your username and hostname. Please check that they are accurate.You can suppress this message by setting them explicitly: git config --global user.name "Your Name" git config --global user.email you@example.comIf the identity used for this commit is wrong, you can fix it with: git commit --amend --author='Your Name &lt;you@example.com&gt;' 2 files changed, 5 insertions(+), 1 deletions(-) create mode 100644 README 跳过使用暂存区域尽管使用暂存区域的方式可以精心准备要提交的细节，但有时候这么做略显繁琐。Git 提供了一个跳过使用暂存区域的方式，只要在提交的时候，给 git commit 加上 -a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤：1git commit -a -m 'new benchmarks' 看到了吗？提交之前不再需要 git add 文件 benchmarks.rb 了。 移除文件要从 Git 中移除某个文件，就必须要从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。可以用 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。如果只是简单地从工作目录中手工删除文件，运行 git status 时就会在 “Changes not staged for commit” 部分（也就是未暂存清单）看到： 12345678910[root@VM_0_8_centos grit]# rm grit.gemspec[root@VM_0_8_centos grit]# git status# On branch master# Changed but not updated:# (use "git add/rm &lt;file&gt;..." to update what will be committed)# (use "git checkout -- &lt;file&gt;..." to discard changes in working directory)## deleted: grit.gemspec#no changes added to commit (use "git add" and/or "git commit -a") 然后再运行 git rm 记录此次移除文件的操作： 123456789[root@VM_0_8_centos grit]# git rm grit.gemspecrm 'grit.gemspec'[root@VM_0_8_centos grit]# git status# On branch master# Changes to be committed:# (use "git reset HEAD &lt;file&gt;..." to unstage)## deleted: grit.gemspec# 最后提交的时候，该文件就不再纳入版本管理了。如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -f（译注：即 force 的首字母），以防误删除文件后丢失修改的内容。 另外一种情况是，我们想把文件从 Git 仓库中删除（亦即从暂存区域移除），但仍然希望保留在当前工作目录中。换句话说，仅是从跟踪清单中删除。比如一些大型日志文件或者一堆 .a 编译文件，不小心纳入仓库后，要移除跟踪但不删除文件，以便稍后在 .gitignore 文件中补上，用 –cached 选项即可： 1$ git rm --cached readme.txt 后面可以列出文件或者目录的名字，也可以使用 glob 模式。比方说： 1$ git rm log/\*.log 注意到星号 * 之前的反斜杠 \，因为 Git 有它自己的文件模式扩展匹配方式，所以我们不用 shell 来帮忙展开（译注：实际上不加反斜杠也可以运行，只不过按照 shell 扩展的话，仅仅删除指定目录下的文件而不会递归匹配。上面的例子本来就指定了目录，所以效果等同，但下面的例子就会用递归方式匹配，所以必须加反斜杠。）。此命令删除所有 log/ 目录下扩展名为 .log 的文件。类似的比如： 1$ git rm \*~ 会递归删除当前目录及其子目录中所有 ~ 结尾的文件。 移动文件不像其他的 VCS 系统，Git 并不跟踪文件移动操作。如果在 Git 中重命名了某个文件，仓库中存储的元数据并不会体现出这是一次改名操作。不过 Git 非常聪明，它会推断出究竟发生了什么，至于具体是如何做到的，我们稍后再谈。 既然如此，当你看到 Git 的 mv 命令时一定会困惑不已。要在 Git 中对文件改名，可以这么做：12345678910[root@VM_0_8_centos mygrit]# git mv README READMINE[root@VM_0_8_centos mygrit]# git status# On branch master# Your branch is ahead of 'origin/master' by 2 commits.## Changes to be committed:# (use "git reset HEAD &lt;file&gt;..." to unstage)## renamed: README -&gt; READMINE# 查看提交历史 在提交了若干更新之后，又或者克隆了某个项目，想回顾下提交历史，可以使用 git log 命令查看。 接下来的例子会用我专门用于演示的 git_learning 项目，运行下面的命令获取该项目源代码：1$ git clone https://github.com/donscoco/git_learning.git 然后在此项目中运行 git log，应该会看到下面的输出： 123456789101112$ git logcommit ffe1cb8e15bba29cbc1883642293c3fd34b421a8Author: donscoco &lt;286563721@qq.com&gt;Date: Wed Nov 28 18:06:45 2018 +0800 git first commitcommit e33bf5386c28b949e3e55c24d7a3ff1a09771e7aAuthor: donscoco &lt;31737198+donscoco@users.noreply.github.com&gt;Date: Wed Nov 28 18:01:52 2018 +0800 Initial commit 默认不用任何参数的话，git log 会按提交时间列出所有的更新，最近的更新排在最上面。看到了吗，每次更新都有一个 SHA-1 校验和、作者的名字和电子邮件地址、提交时间，最后缩进一个段落显示提交说明。 git log 有许多选项可以帮助你搜寻感兴趣的提交，接下来我们介绍些最常用的。 我们常用 -p 选项展开显示每次提交的内容差异，用 -2 则仅显示最近的两次更新： 123456789101112131415161718192021222324$ git log -p -2commit ffe1cb8e15bba29cbc1883642293c3fd34b421a8Author: donscoco &lt;286563721@qq.com&gt;Date: Wed Nov 28 18:06:45 2018 +0800 git first commitdiff --git a/README.md b/README.mdindex cdab949..2694e5c 100644--- a/README.md+++ b/README.md@@ -1,2 +1,3 @@ # git_learning 学习git+# git_learning_test_1commit e33bf5386c28b949e3e55c24d7a3ff1a09771e7aAuthor: donscoco &lt;31737198+donscoco@users.noreply.github.com&gt;Date: Wed Nov 28 18:01:52 2018 +0800 Initial commitdiff --git a/README.md b/README.mdnew file mode 100644 在做代码审查，或者要快速浏览其他协作者提交的更新都作了哪些改动时，就可以用这个选项。此外，还有许多摘要选项可以用，比如 –stat，仅显示简要的增改行数统计： 12345```每个提交都列出了修改过的文件，以及其中添加和移除的行数，并在最后列出所有增减行数小计。还有个常用的 --pretty 选项，可以指定使用完全不同于默认格式的方式展示提交历史。比如用 oneline 将每个提交放在一行显示，这在提交数很大时非常有用。另外还有 short，full 和 fuller 可以用，展示的信息或多或少有些不同，请自己动手实践一下看看效果如何。```bash 但最有意思的是 format，可以定制要显示的记录格式，这样的输出便于后期编程提取分析，像这样： 123456789101112131415161718192021```列出了常用的格式占位符写法及其代表的意义。```bash选项 说明 %H 提交对象（commit）的完整哈希字串 %h 提交对象的简短哈希字串 %T 树对象（tree）的完整哈希字串 %t 树对象的简短哈希字串 %P 父对象（parent）的完整哈希字串 %p 父对象的简短哈希字串 %an 作者（author）的名字 %ae 作者的电子邮件地址 %ad 作者修订日期（可以用 -date= 选项定制格式） %ar 作者修订日期，按多久以前的方式显示 %cn 提交者(committer)的名字 %ce 提交者的电子邮件地址 %cd 提交日期 %cr 提交日期，按多久以前的方式显示 %s 提交说明 你一定奇怪作者（author）和提交者（committer）之间究竟有何差别，其实作者指的是实际作出修改的人，提交者指的是最后将此工作成果提交到仓库的人。所以，当你为某个项目发布补丁，然后某个核心成员将你的补丁并入项目时，你就是作者，而那个核心成员就是提交者。我们会在第五章再详细介绍两者之间的细微差别。 用 oneline 或 format 时结合 –graph 选项，可以看到开头多出一些 ASCII 字符串表示的简单图形，形象地展示了每个提交所在的分支及其分化衍合情况。在我们之前提到的 Grit 项目仓库中可以看到： 123456789101112131415```以上只是简单介绍了一些 git log 命令支持的选项。表 2-2 还列出了一些其他常用的选项及其释义。```bash选项 说明 -p 按补丁格式显示每个更新之间的差异。 --stat 显示每次更新的文件修改统计信息。 --shortstat 只显示 --stat 中最后的行数修改添加移除统计。 --name-only 仅在提交信息后显示已修改的文件清单。 --name-status 显示新增、修改、删除的文件清单。 --abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。 --relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）。 --graph 显示 ASCII 图形表示的分支合并历史。 --pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）。 限制输出长度除了定制输出格式的选项之外，git log 还有许多非常实用的限制输出长度的选项，也就是只输出部分提交信息。之前我们已经看到过 -2 了，它只显示最近的两条提交，实际上，这是 - 选项的写法，其中的 n 可以是任何自然数，表示仅显示最近的若干条提交。不过实践中我们是不太用这个选项的，Git 在输出所有提交时会自动调用分页程序（less），要看更早的更新只需翻到下页即可。 另外还有按照时间作限制的选项，比如 –since 和 –until。下面的命令列出所有最近两周内的提交： 1git log --since=2.weeks 你可以给出各种时间格式，比如说具体的某一天（“2008-01-15”），或者是多久以前（“2 years 1 day 3 minutes ago”）。 还可以给出若干搜索条件，列出符合的提交。用 –author 选项显示指定作者的提交，用 –grep 选项搜索提交说明中的关键字。（请注意，如果要得到同时满足这两个选项搜索条件的提交，就必须用 –all-match 选项。否则，满足任意一个条件的提交都会被匹配出来） 另一个真正实用的git log选项是路径(path)，如果只关心某些文件或者目录的历史提交，可以在 git log 选项的最后指定它们的路径。因为是放在最后位置上的选项，所以用两个短划线（–）隔开之前的选项和后面限定的路径名。 表 还列出了其他常用的类似选项。 123456选项 说明 -(n) 仅显示最近的 n 条提交 --since, --after 仅显示指定时间之后的提交。 --until, --before 仅显示指定时间之前的提交。 --author 仅显示指定作者相关的提交。 --committer 仅显示指定提交者相关的提交。 来看一个实际的例子，如果要查看 Git 仓库中，2008 年 10 月期间，Junio Hamano 提交的但未合并的测试脚本（位于项目的 t/ 目录下的文件），可以用下面的查询命令： 12345678$ git log --pretty="%h - %s" --author=gitster --since="2008-10-01" \ --before="2008-11-01" --no-merges -- t/ 5610e3b - Fix testcase failure when extended attribute acd3b9e - Enhance hold_lock_file_for_&#123;update,append&#125;() f563754 - demonstrate breakage of detached checkout wi d1a43f2 - reset --hard/read-tree --reset -u: remove un 51a94af - Fix "checkout --track -b newbranch" on detac b0ad11e - pull: allow "git pull origin $something:$cur Git 项目有 20,000 多条提交，但我们给出搜索选项后，仅列出了其中满足条件的 6 条。 使用图形化工具查阅提交历史有时候图形化工具更容易展示历史提交的变化，随 Git 一同发布的 gitk 就是这样一种工具。它是用 Tcl/Tk 写成的，基本上相当于 git log 命令的可视化版本，凡是 git log 可以用的选项也都能用在 gitk 上。在项目工作目录中输入 gitk 命令后，就会启动图所示的界面。 上半个窗口显示的是历次提交的分支祖先图谱，下半个窗口显示当前点选的提交对应的具体差异。 撤消操作 任何时候，你都有可能需要撤消刚才所做的某些操作。接下来，我们会介绍一些基本的撤消操作相关的命令。请注意，有些撤销操作是不可逆的，所以请务必谨慎小心，一旦失误，就有可能丢失部分工作成果。 修改最后一次提交有时候我们提交完了才发现漏掉了几个文件没有加，或者提交信息写错了。想要撤消刚才的提交操作，可以使用 –amend 选项重新提交：1$ git commit --amend 此命令将使用当前的暂存区域快照提交。如果刚才提交完没有作任何改动，直接运行此命令的话，相当于有机会重新编辑提交说明，但将要提交的文件快照和之前的一样。 启动文本编辑器后，会看到上次提交时的说明，编辑它确认没问题后保存退出，就会使用新的提交说明覆盖刚才失误的提交。 如果刚才提交时忘了暂存某些修改，可以先补上暂存操作，然后再运行 –amend 提交： 123$ git commit -m 'initial commit'$ git add forgotten_file$ git commit --amend 上面的三条命令最终只是产生一个提交，第二个提交命令修正了第一个的提交内容。 取消已经暂存的文件接下来的两个小节将演示如何取消暂存区域中的文件，以及如何取消工作目录中已修改的文件。不用担心，查看文件状态的时候就提示了该如何撤消，所以不需要死记硬背。来看下面的例子，有两个修改过的文件，我们想要分开提交，但不小心用 git add . 全加到了暂存区域。该如何撤消暂存其中的一个文件呢？其实，git status 的命令输出已经告诉了我们该怎么做： 123456# Changes not staged for commit: # (use "git add &lt;file&gt;..." to update what will be committed) # (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) # # modified: benchmarks.rb # 在第二个括号中，我们看到了抛弃文件修改的命令（至少在 Git 1.6.1 以及更高版本中会这样提示，如果你还在用老版本，我们强烈建议你升级，以获取最佳的用户体验），让我们试试看：12345678$ git checkout -- benchmarks.rb $ git status # On branch master # Changes to be committed: # (use "git reset HEAD &lt;file&gt;..." to unstage) # # modified: README.txt # 可以看到，该文件已经恢复到修改前的版本。你可能已经意识到了，这条命令有些危险，所有对文件的修改都没有了，因为我们刚刚把之前版本的文件复制过来重写了此文件。所以在用这条命令前，请务必确定真的不再需要保留刚才的修改。如果只是想回退版本，同时保留刚才的修改以便将来继续工作，可以用下章介绍的 stashing 和分支来处理，应该会更好些。 记住，任何已经提交到 Git 的都可以被恢复。即便在已经删除的分支中的提交，或者用 –amend 重新改写的提交，都可以被恢复（关于数据恢复的内容见第九章）。所以，你可能失去的数据，仅限于没有提交过的，对 Git 来说它们就像从未存在过一样。 远程仓库的使用 要参与任何一个 Git 项目的协作，必须要了解该如何管理远程仓库。远程仓库是指托管在网络上的项目仓库，可能会有好多个，其中有些你只能读，另外有些可以写。同他人协作开发某个项目时，需要管理这些远程仓库，以便推送或拉取数据，分享各自的工作进展。管理远程仓库的工作，包括添加远程库，移除废弃的远程库，管理各式远程库分支，定义是否跟踪这些分支，等等。本节我们将详细讨论远程库的管理和使用。 添加远程仓库要添加一个新的远程仓库，可以指定一个简单的名字，以便将来引用，运行 git remote add [shortname] [url]：1$ git remote add origin https://github.com/donscoco/git_learning.git 现在可以用字符串 pb 指代对应的仓库地址了。比如说，要抓取所有 Paul 有的，但本地仓库没有的信息，可以运行 git fetch pb：12345678$ git fetch pb remote: Counting objects: 58, done. remote: Compressing objects: 100% (41/41), done. remote: Total 44 (delta 24), reused 1 (delta 0) Unpacking objects: 100% (44/44), done. From git://github.com/paulboone/ticgit * [new branch] master -&gt; pb/master * [new branch] ticgit -&gt; pb/ticgit 现在，Paul 的主干分支（master）已经完全可以在本地访问了，对应的名字是 pb/master，你可以将它合并到自己的某个分支，或者切换到这个分支，看看有些什么有趣的更新。 从远程仓库抓取数据正如之前所看到的，可以用下面的命令从远程仓库抓取数据到本地：1$ git fetch [remote-name] 此命令会到远程仓库中拉取所有你本地仓库中还没有的数据。运行完成后，你就可以在本地访问该远程仓库中的所有分支，将其中某个分支合并到本地，或者只是取出某个分支，一探究竟。（我们会在第三章详细讨论关于分支的概念和操作。） 如果是克隆了一个仓库，此命令会自动将远程仓库归于 origin 名下。所以，git fetch origin 会抓取从你上次克隆以来别人上传到此远程仓库中的所有更新（或是上次 fetch 以来别人提交的更新）。有一点很重要，需要记住，fetch 命令只是将远端的数据拉到本地仓库，并不自动合并到当前工作分支，只有当你确实准备好了，才能手工合并。 如果设置了某个分支用于跟踪某个远端仓库的分支（参见下节及第三章的内容），可以使用 git pull 命令自动抓取数据下来，然后将远端分支自动合并到本地仓库中当前分支。在日常工作中我们经常这么用，既快且好。实际上，默认情况下 git clone 命令本质上就是自动创建了本地的 master 分支用于跟踪远程仓库中的 master 分支（假设远程仓库确实有 master 分支）。所以一般我们运行 git pull，目的都是要从原始克隆的远端仓库中抓取数据后，合并到工作目录中的当前分支。 推送数据到远程仓库项目进行到一个阶段，要同别人分享目前的成果，可以将本地仓库中的数据推送到远程仓库。实现这个任务的命令很简单： git push [remote-name] [branch-name]。如果要把本地的 master 分支推送到 origin 服务器上（再次说明下，克隆操作会自动使用默认的 master 和 origin 名字），可以运行下面的命令：1$ git push origin master 只有在所克隆的服务器上有写权限，或者同一时刻没有其他人在推数据，这条命令才会如期完成任务。如果在你推数据前，已经有其他人推送了若干更新，那你的推送操作就会被驳回。你必须先把他们的更新抓取到本地，合并到自己的项目中，然后才可以再次推送。有关推送数据到远程仓库的详细内容见第三章。 查看当前的远程库要查看当前配置有哪些远程仓库，可以用 git remote 命令，它会列出每个远程库的简短名字。在克隆完某个项目后，至少可以看到一个名为 origin 的远程库，Git 默认使用这个名字来标识你所克隆的原始仓库：12$ git remoteorigin 也可以加上 -v 选项（译注：此为 –verbose 的简写，取首字母），显示对应的克隆地址： 123$ git remote -vorigin https://github.com/donscoco/git_learning.git (fetch)origin https://github.com/donscoco/git_learning.git (push) 查看远程仓库信息我们可以通过命令 git remote show [remote-name] 查看某个远程仓库的详细信息，比如要看所克隆的 origin 仓库，可以运行： 1234567891011$ git remote show origin* remote origin Fetch URL: https://github.com/donscoco/git_learning.git Push URL: https://github.com/donscoco/git_learning.git HEAD branch: master Remote branch: master tracked Local branch configured for 'git pull': master merges with remote master Local ref configured for 'git push': master pushes to master (local out of date) 除了对应的克隆地址外，它还给出了许多额外的信息。它友善地告诉你如果是在 master 分支，就可以用 git pull 命令抓取数据合并到本地。另外还列出了所有处于跟踪状态中的远端分支。 上面的例子非常简单，而随着使用 Git 的深入，git remote show 给出的信息可能会像这样： 123456789101112131415161718192021$ git remote show origin * remote origin URL: git@github.com:defunkt/github.git Remote branch merged with 'git pull' while on branch issues issues Remote branch merged with 'git pull' while on branch master master New remote branches (next fetch will store in remotes/origin) caching Stale tracking branches (use 'git remote prune') libwalker walker2 Tracked remote branches acl apiv2 dashboard2 issues master postgres Local branch pushed with 'git push' master:master 它告诉我们，运行 git push 时缺省推送的分支是什么（译注：最后两行）。它还显示了有哪些远端分支还没有同步到本地（译注：第六行的 caching 分支），哪些已同步到本地的远端分支在远端服务器上已被删除（译注：Stale tracking branches 下面的两个分支），以及运行 git pull 时将自动合并哪些分支（译注：前四行中列出的 issues 和 master 分支）。 远程仓库的删除和重命名在新版 Git 中可以用 git remote rename 命令修改某个远程仓库在本地的简称，比如想把 pb 改成 paul，可以这么运行：1234$ git remote rename pb paul $ git remote origin paul 注意，对远程仓库的重命名，也会使对应的分支名称发生变化，原来的 pb/master 分支现在成了 paul/master。 碰到远端仓库服务器迁移，或者原来的克隆镜像不再使用，又或者某个参与者不再贡献代码，那么需要移除对应的远端仓库，可以运行 git remote rm 命令： 123$ git remote rm paul$ git remote origin 打标签 同大多数 VCS 一样，Git 也可以对某一时间点上的版本打上标签。人们在发布某个软件版本（比如 v1.0 等等）的时候，经常这么做。本节我们一起来学习如何列出所有可用的标签，如何新建标签，以及各种不同类型标签之间的差别。 列显已有的标签列出现有标签的命令非常简单，直接运行 git tag 即可：12345```显示的标签按字母顺序排列，所以标签的先后并不表示重要程度的轻重。我们可以用特定的搜索模式列出符合条件的标签。在 Git 自身项目仓库中，有着超过 240 个标签，如果你只对 1.4.2 系列的版本感兴趣，可以运行下面的命令：```bash 新建标签Git 使用的标签有两种类型：轻量级的（lightweight）和含附注的（annotated）。轻量级标签就像是个不会变化的分支，实际上它就是个指向特定提交对象的引用。而含附注标签，实际上是存储在仓库中的一个独立对象，它有自身的校验和信息，包含着标签的名字，电子邮件地址和日期，以及标签说明，标签本身也允许使用 GNU Privacy Guard (GPG) 来签署或验证。一般我们都建议使用含附注型的标签，以便保留相关信息；当然，如果只是临时性加注标签，或者不需要旁注额外信息，用轻量级标签也没问题。 含附注的标签创建一个含附注类型的标签非常简单，用 -a （译注：取 annotated 的首字母）指定标签名字即可： 123456789101112131415161718```而 -m 选项则指定了对应的标签说明，Git 会将此说明一同保存在标签对象中。如果没有给出该选项，Git 会启动文本编辑软件供你输入标签说明。可以使用 git show 命令查看相应标签的版本信息，并连同显示打标签时的提交对象。```bash$ git show v1.4 tag v1.4 Tagger: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Mon Feb 9 14:45:11 2009 -0800 my version 1.4 commit 15027957951b64cf874c3557a0f3547bd83b3ff6 Merge: 4a447f7... a6b4c97... Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Sun Feb 8 19:02:46 2009 -0800 Merge branch 'experiment' 我们可以看到在提交对象信息上面，列出了此标签的提交者和提交时间，以及相应的标签说明。 签署标签如果你有自己的私钥，还可以用 GPG 来签署标签，只需要把之前的 -a 改为 -s （译注： 取 signed 的首字母）即可：1234$ git tag -s v1.5 -m 'my signed 1.5 tag' You need a passphrase to unlock the secret key for user: "Scott Chacon &lt;schacon@gee-mail.com&gt;" 1024-bit DSA key, ID F721C45A, created 2009-02-09 现在再运行 git show 会看到对应的 GPG 签名也附在其内：12345678910111213141516171819$ git show v1.5 tag v1.5 Tagger: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Mon Feb 9 15:22:20 2009 -0800 my signed 1.5 tag -----BEGIN PGP SIGNATURE----- Version: GnuPG v1.4.8 (Darwin) iEYEABECAAYFAkmQurIACgkQON3DxfchxFr5cACeIMN+ZxLKggJQf0QYiQBwgySN Ki0An2JeAVUCAiJ7Ox6ZEtK+NvZAj82/ =WryJ -----END PGP SIGNATURE----- commit 15027957951b64cf874c3557a0f3547bd83b3ff6 Merge: 4a447f7... a6b4c97... Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Sun Feb 8 19:02:46 2009 -0800 Merge branch 'experiment' 稍后我们再学习如何验证已经签署的标签。 轻量级标签轻量级标签实际上就是一个保存着对应提交对象的校验和信息的文件。要创建这样的标签，一个 -a，-s 或 -m 选项都不用，直接给出标签名字即可： 1234567$ git tag v1.4-lw $ git tag v0.1 v1.3 v1.4 v1.4-lw v1.5 现在运行 git show 查看此标签信息，就只有相应的提交对象摘要： 1234567$ git show v1.4-lw commit 15027957951b64cf874c3557a0f3547bd83b3ff6 Merge: 4a447f7... a6b4c97... Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Sun Feb 8 19:02:46 2009 -0800 Merge branch 'experiment' 验证标签可以使用 git tag -v [tag-name] （译注：取 verify 的首字母）的方式验证已经签署的标签。此命令会调用 GPG 来验证签名，所以你需要有签署者的公钥，存放在 keyring 中，才能验证： 12345678910111213$ git tag -v v1.4.2.1 object 883653babd8ee7ea23e6a5c392bb739348b1eb61 type commit tag v1.4.2.1 tagger Junio C Hamano &lt;junkio@cox.net&gt; 1158138501 -0700 GIT 1.4.2.1 Minor fixes since 1.4.2, including git-mv and git-http with alternates. gpg: Signature made Wed Sep 13 02:08:25 2006 PDT using DSA key ID F3119B9A gpg: Good signature from "Junio C Hamano &lt;junkio@cox.net&gt;" gpg: aka "[jpeg image of size 1513]" Primary key fingerprint: 3565 2A26 2040 E066 C9A7 4A7D C0C6 D9A4 F311 9B9A 若是没有签署者的公钥，会报告类似下面这样的错误：123gpg: Signature made Wed Sep 13 02:08:25 2006 PDT using DSA key ID F3119B9A gpg: Can't check signature: public key not found error: could not verify the tag 'v1.4.2.1' 后期加注标签你甚至可以在后期对早先的某次提交加注标签。比如在下面展示的提交历史中：1234567891011$ git log --pretty=oneline 15027957951b64cf874c3557a0f3547bd83b3ff6 Merge branch 'experiment' a6b4c97498bd301d84096da251c98a07c7723e65 beginning write support 0d52aaab4479697da7686c15f77a3d64d9165190 one more thing 6d52a271eda8725415634dd79daabbc4d9b6008e Merge branch 'experiment' 0b7434d86859cc7b8c3d5e1dddfed66ff742fcbc added a commit function 4682c3261057305bdd616e23b64b0857d832627b added a todo file 166ae0c4d3f420721acbb115cc33848dfcc2121a started write support 9fceb02d0ae598e95dc970b74767f19372d61af8 updated rakefile 964f16d36dfccde844893cac5b347e7b3d44abbc commit the todo 8a5cbc430f1a9c3d00faaeffd07798508422908a updated readme 我们忘了在提交 “updated rakefile” 后为此项目打上版本号 v1.2，没关系，现在也能做。只要在打标签的时候跟上对应提交对象的校验和（或前几位字符）即可： 1$ git tag -a v1.2 9fceb02 可以看到我们已经补上了标签： 1234567891011121314151617181920$ git tag v0.1 v1.2 v1.3 v1.4 v1.4-lw v1.5 $ git show v1.2 tag v1.2 Tagger: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Mon Feb 9 15:32:16 2009 -0800 version 1.2 commit 9fceb02d0ae598e95dc970b74767f19372d61af8 Author: Magnus Chacon &lt;mchacon@gee-mail.com&gt; Date: Sun Apr 27 20:43:35 2008 -0700 updated rakefile ... 分享标签默认情况下，git push 并不会把标签传送到远端服务器上，只有通过显式命令才能分享标签到远端仓库。其命令格式如同推送分支，运行 git push origin [tagname] 即可： 1234567$ git push origin v1.5 Counting objects: 50, done. Compressing objects: 100% (38/38), done. Writing objects: 100% (44/44), 4.56 KiB, done. Total 44 (delta 18), reused 8 (delta 1) To git@github.com:schacon/simplegit.git * [new tag] v1.5 -&gt; v1.5 如果要一次推送所有本地新增的标签上去，可以使用 –tags 选项： 1234567891011$ git push origin --tags Counting objects: 50, done. Compressing objects: 100% (38/38), done. Writing objects: 100% (44/44), 4.56 KiB, done. Total 44 (delta 18), reused 8 (delta 1) To git@github.com:schacon/simplegit.git * [new tag] v0.1 -&gt; v0.1 * [new tag] v1.2 -&gt; v1.2 * [new tag] v1.4 -&gt; v1.4 * [new tag] v1.4-lw -&gt; v1.4-lw * [new tag] v1.5 -&gt; v1.5 现在，其他人克隆共享仓库或拉取数据同步后，也会看到这些标签。 技巧和窍门 在结束本章之前，我还想和大家分享一些 Git 使用的技巧和窍门。很多使用 Git 的开发者可能根本就没用过这些技巧，我们也不是说在读过本书后非得用这些技巧不可，但至少应该有所了解吧。说实话，有了这些小窍门，我们的工作可以变得更简单，更轻松，更高效。 自动补全如果你用的是 Bash shell，可以试试看 Git 提供的自动补全脚本。下载 Git 的源代码，进入 contrib/completion 目录，会看到一个 git-completion.bash 文件。将此文件复制到你自己的用户主目录中（译注：按照下面的示例，还应改名加上点：cp git-completion.bash ~/.git-completion.bash），并把下面一行内容添加到你的 .bashrc 文件中： 1source ~/.git-completion.bash 也可以为系统上所有用户都设置默认使用此脚本。Mac 上将此脚本复制到 /opt/local/etc/bash_completion.d 目录中，Linux 上则复制到 /etc/bash_completion.d/ 目录中。这两处目录中的脚本，都会在 Bash 启动时自动加载。 如果在 Windows 上安装了 msysGit，默认使用的 Git Bash 就已经配好了这个自动补全脚本，可以直接使用。 在输入 Git 命令的时候可以敲两次跳格键（Tab），就会看到列出所有匹配的可用命令建议：12$ git co&lt;tab&gt;&lt;tab&gt; commit config 此例中，键入 git co 然后连按两次 Tab 键，会看到两个相关的建议（命令） commit 和 config。继而输入 m 会自动完成 git commit 命令的输入。 命令的选项也可以用这种方式自动完成，其实这种情况更实用些。比如运行 git log 的时候忘了相关选项的名字，可以输入开头的几个字母，然后敲 Tab 键看看有哪些匹配的： 12$ git log --s&lt;tab&gt; --shortstat --since= --src-prefix= --stat --summary 这个技巧不错吧，可以节省很多输入和查阅文档的时间。 Git 命令别名Git 并不会推断你输入的几个字符将会是哪条命令，不过如果想偷懒，少敲几个命令的字符，可以用 git config 为命令设置别名。来看看下面的例子：1234$ git config --global alias.co checkout $ git config --global alias.br branch $ git config --global alias.ci commit $ git config --global alias.st status 现在，如果要输入 git commit 只需键入 git ci 即可。而随着 Git 使用的深入，会有很多经常要用到的命令，遇到这种情况，不妨建个别名提高效率。 使用这种技术还可以创造出新的命令，比方说取消暂存文件时的输入比较繁琐，可以自己设置一下： 1$ git config --global alias.unstage 'reset HEAD --' 这样一来，下面的两条命令完全等同：12$ git unstage fileA $ git reset HEAD fileA 显然，使用别名的方式看起来更清楚。另外，我们还经常设置 last 命令：1$ git config --global alias.last 'log -1 HEAD' 然后要看最后一次的提交信息，就变得简单多了： 12345678$ git last commit 66938dae3329c7aebe598c2246a8e6af90d04646 Author: Josh Goebel &lt;dreamer3@example.com&gt; Date: Tue Aug 26 19:48:51 2008 +0800 test for current head Signed-off-by: Scott Chacon &lt;schacon@example.com&gt; 可以看出，实际上 Git 只是简单地在命令中替换了你设置的别名。不过有时候我们希望运行某个外部命令，而非 Git 的子命令，这个好办，只需要在命令前加上 ! 就行。如果你自己写了些处理 Git 仓库信息的脚本的话，就可以用这种技术包装起来。作为演示，我们可以设置用 git visual 启动 gitk： 1$ git config --global alias.visual '!gitk' Git 分支几乎每一种版本控制系统都以某种形式支持分支。使用分支意味着你可以从开发主线上分离开来，然后在不影响主线的同时继续工作。在很多版本控制系统中，这是个昂贵的过程，常常需要创建一个源代码目录的完整副本，对大型项目来说会花费很长时间。 有人把 Git 的分支模型称为“必杀技特性”，而正是因为它，将 Git 从版本控制系统家族里区分出来。Git 有何特别之处呢？Git 的分支可谓是难以置信的轻量级，它的新建操作几乎可以在瞬间完成，并且在不同分支间切换起来也差不多一样快。和许多其他版本控制系统不同，Git 鼓励在工作流程中频繁使用分支与合并，哪怕一天之内进行许多次都没有关系。理解分支的概念并熟练运用后，你才会意识到为什么 Git 是一个如此强大而独特的工具，并从此真正改变你的开发方式。 何谓分支123``````bash 123``````bash]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux系统管理]]></title>
    <url>%2F2018%2F11%2F24%2FLinux%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[进程管理什么是进程？进程（英语：process），是指计算机中已运行的程序。进程为曾经是分时系统的基本运作单位。在面向进程设计的系统（如早期的UNIX，Linux 2.4及更早的版本）中，进程是程序的基本执行实体；在面向线程设计的系统（如当代多数操作系统、Linux 2.6及更新的版本）中，进程本身不是基本运行单位，而是线程的容器。程序本身只是指令、数据及其组织形式的描述，进程才是程序（那些指令和数据）的真正运行实例。若干进程有可能与同一个程序相关系，且每个进程皆可以同步（循序）或异步（平行）的方式独立运行。现代计算机系统可在同一段时间内以进程的形式将多个程序加载到存储器中，并借由时间共享（或称时分复用），以在一个处理器上表现出同时（平行性）运行的感觉。同样的，使用多线程技术（多线程即每一个线程都代表一个进程内的一个独立执行上下文）的操作系统或计算机体系结构，同样程序的平行线程，可在多CPU主机或网络上真正同时运行（在不同的CPU上）。以上来自 wiki 查看进程ps aux 或者 ps -le||||-|-||USER|产生进程的用户||PID|进程的ID||%CPU|进程占用的CPU百分比||%MEM|进程占用的内存百分比||VSZ|进程占用的虚拟内存量(KB)||RSS|进程占用的实际内存量(KB)||TTY|该进程是在哪个终端中运行的，tty1-tty6代表本地控制台字符终端，ps/0-255代表虚拟终端||STAT|进程的状态；S:睡眠，R:运行，T:停止，s:包含子进程，+:位于后台||TIME|进程实际使用的CPU时间||COMMAND|产生进程的命令名| 查看进程树pstree [option]-p 进程id-u 进程的所属用户 12345678910111213141516171819202122[root@VM_0_8_centos ~]# pstreeinit-+-abrtd |-acpid |-agetty |-atd |-auditd---&#123;auditd&#125; |-crond |-dbus-daemon |-ddgs.3014---8*[&#123;ddgs.3014&#125;] |-dhclient |-master-+-bounce | |-pickup | |-qmgr | `-smtp |-6*[mingetty] |-ntpd |-qW3xT.4---5*[&#123;qW3xT.4&#125;] |-sshd-+-21*[sshd---sshd] | |-sshd---bash | `-sshd---bash---pstree `-udevd---2*[udevd][root@VM_0_8_centos ~]# 查看系统健康状态top [option]-d 秒数：指定top每隔几秒刷新，默认3秒-b ：使用批处理模式更新-n 次数：指定top命令执行的次数交互界面中?或者h获取帮助P以CPU使用率排序M以内存使用量排序N以PID排序q退出交互界面 12345top - 20:00:34 up 4:20, 2 users, load average: 0.00, 0.00, 0.00Tasks: 102 total, 1 running, 101 sleeping, 0 stopped, 0 zombieCpu(s): 0.0%us, 0.3%sy, 0.0%ni, 99.7%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stMem: 2054224k total, 298700k used, 1755524k free, 15916k buffersSwap: 0k total, 0k used, 0k free, 188420k cached 交互界面中这个是判断系统健康状态最重要的指标 20:00:34系统时间up 4:20系统启动时长2 users当前登陆用户数量load average: 0.00, 0.00, 0.00系统在1分钟前，5分钟前，15分钟前的平均负载Tasks: 102 total, 1 running, 101 sleeping, 0 stopped, 0 zombie总共的进程数,1个在运行，101个睡眠，0停止，0僵尸僵尸进程：某些进程a依赖其他进程b，其他b死了，进程a卡住了，a就是僵尸进程，一般僵尸进程过一段时间就会自己停止Cpu(s): 0.0%us, 0.3%sy, 0.0%ni, 99.7%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%st CPU 用户占用百分比，系统占用百分比，被修改过优先级的进程占用百分比，空闲百分比(id);Mem: 2054224k total, 298700k used, 1755524k free, 15916k buffers 内存总数，使用数，剩余数，缓冲。Swap: 0k total, 0k used, 0k free, 188420k cached 交换分区占比这里注意 buffers是缓冲–加速数据写入（先缓冲到内存，再写到硬盘上），cached是缓存–加速数据读取（先缓存到交换区，cpu不用一直等） 这里重点要看的就是 load average,CPU的空闲id（如果空闲国小低于20%，那么系统一般会比较卡）,Mem的空闲（内存快不够了）,swap的交换占比 常用命令top -b -n 1 &gt; /root/top.log 打印一次结果存放到日志 杀死进程killkill -l –查看信号 killallkillall [选项][信号] 进程名-i 交互式，每个进程询问是否要杀-I 忽略进程名大小写 pkillpkill [option][sin] 进程名-t 可以按照终端号踢出用户pkill -9 -t pts/1 修改进程的优先级什么是进程的优先级PRI NI用户不能直接改PRI，系统最后决定的优先级 pri+ni ,越小优先级越大 修改nice [option] order例如：nice -n -5 service httpd startrenice [priority] PIDrenice -10 2125 工作管理什么是工作管理工作管理指单个终端中管理多个工作 查后台运行的工作jobs [-l]-l 显示工作的id12345[root@VM_0_8_centos ~]# jobs[1] Stopped top[2]- Stopped top[3]+ Stopped tar jdk-7u80-linux-x64.rpm ./[4] Running wget http://download.oracle.com/otn-pub/java/jdk/7u67-b01/server-jre-7u67-linux-x64.tar &amp; +是最后一个加入的，-是倒数第二个 把命令放入后台 在命令后面加&amp;符放在后台执行 按下ctrl+z 放在后台暂停 后台工作回复bg %工作号 – backgroundbg –什么都不加就恢复 +号 的工作 后台任务脱离终端运行把命令放入后台，只能在当前登陆终端执行。一旦退出或者关闭终端，后台程序将会停止。其实就是进程绑定在终端，当终端退出，绑定的命令进程就会退出。而例如 mysql 服务就算关闭终端也不会停止 ，是因为mysql是把进程启动为了守护进程。 rc.local把需要后台执行的命令加入/etc/rc.local文件，文件是在启动时运行 系统定时任务nohupnohub [命令] &amp; 系统资源查看vmstat监控系统资源vmstat [刷新延时] [刷新次数] 123456[root@VM_0_8_centos ~]# vmstat 1 3procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 2 0 0 1179156 190320 276284 0 0 1 9 139 49 64 0 35 0 0 1 0 0 1178660 190320 276308 0 0 0 0 1021 55 100 0 0 0 0 1 0 0 1178224 190320 276308 0 0 0 0 1015 34 100 0 0 0 0 procs 进程信息字段:r是等待运行的进程数，b是不可被唤醒的进程数量memory内存信息字段：单位(KB) swapd: 虚拟内存使用情况 free: 空闲内存使用容量 buff: 缓冲内存容量 cache: 缓存内存容量 swap 交换分区信息字段: 单位(KB) ; 这两个值越大，说明数据需要经常在硬盘和内存中交换，系统性能差 si: 从磁盘交换到内存中数据的数量 so: 从内存中交换到磁盘中数据的数量 io 磁盘读写信息字段 单位是块；这两个值越大，说明系统I/O越繁忙 bi: 从块设备读入数据的总量，单位是块 bo: 写入到块设备的数据总量，单位是块 system系统信息字段: 这两个值越大，说明系统与接口设备的通信非常繁忙 in: 每秒被中断的进程次数 cs: 每秒钟进行的事件切换次数 cpu CPU信息字段 us: 非内核进程消耗CPU运算时间的百分比 sy: 内核进程消耗CPU运算时间的百分比 id: 空闲CPU百分比 wa: 等待I/O所消耗的百分比 st: 被虚拟机盗用的CPU百分比 dmesg开机时内核检测信息dmesgdmesg | grep CPU常用123456789[root@VM_0_8_centos ~]# dmesg | grep CPUSMP: Allowing 1 CPUs, 0 hotplug CPUsNR_CPUS:4096 nr_cpumask_bits:1 nr_cpu_ids:1 nr_node_ids:1PERCPU: Embedded 31 pages/cpu @ffff880002200000 s96600 r8192 d22184 u2097152[ 0.029333] CPU: Unsupported number of siblings 14[ 0.031096] mce: CPU supports 10 MCE banks[ 0.064996] CPU0: Intel(R) Xeon(R) CPU E5-26xx v4 stepping 01[ 0.067993] Performance Events: unsupported p6 CPU model 79 no PMU driver, software events only.[ 0.072012] Brought up 1 CPUs free查看内存使用状态free [option] -b: 以字节单位显示 -k: 以KV单位显示 -m: 以MB单位显示 -g: 以GB单位显示 12345[root@VM_0_8_centos ~]# free total used free shared buffers cachedMem: 1922268 754728 1167540 1252 190440 278968-/+ buffers/cache: 285320 1636948Swap: 0 0 0 这里第二行是算上 buffers和cached的12used (line 1) = used (line 2) - buffers - cachedfree (line 1) = free (line 2) + buffers + cached 缓存和缓冲的区别：缓存cache 是用来加速数据从硬盘“读取”的–需要使用的数据，从硬盘先让他缓存到内存，因为硬盘速度慢换冲buffer 是用来加速数据“写入”磁盘的–需要保存的数据，先让他缓存到内存一定数量再一起写入，因为硬盘速度慢 关于swap的概念：Linux 将物理内存分为内存段，叫做页面。交换是指内存页面被复制到预先设定好的硬盘空间(叫做交换空间)的过程，目的是释放对于页面的内存。物理内存和交换空间的总大小是可用的虚拟内存的总量。交换空间通常是一个磁盘分区，但是也可以是一个文件。用户可以在安装 Arch Linux 的时候创建交换空间，或者在安装后的任何时间建立交换空间。对于 RAM 小于 1GB 的用户，交换空间通常是推荐的，但是对于拥有大量的物理内存的用户来说是否使用主要看个人口味了(尽管它对于休眠到硬盘支持是必须的)。要检查交换空间的状态，使用：1$ swapon -s 想安装一个 Linux 交换区域，使用 mkswap 命令例如 mkswap /dev/sdxy 警告: 指定分区上的所有数据会丢失。想要启用一个设备作为交换分区，使用 swapon例如 swapon /dev/sdxy 更多swap相关参考https://wiki.archlinux.org/index.php/Swap_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87) 查看CPU信息cat /proc/cpuinfo 显示系统的启动时间和平均负载uptime 12[root@VM_0_8_centos ~]# uptime 13:03:40 up 2 days, 19:31, 1 user, load average: 1.00, 1.00, 1.00 同top和w的第一行 查看系统与内核相关信息uname [option] -a: 查看系统所有相关信息 -r: 查看内核版本 -s: 查看内核名称 查看系统操作位数file /bin/ls其实就是查看任意一个命令的信息 12[root@VM_0_8_centos ~]# file /bin/ls/bin/ls: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, stripped 查询当前系统发现版本lsb_release -a 123456[root@iZzf6hdnuqzu7wZ ~]# lsb_release -aLSB Version: :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarchDistributor ID: CentOSDescription: CentOS release 6.8 (Final)Release: 6.8Codename: Final 列出进程打开或使用的文件信息lsof [option]列出进程调用或打开的文件信息 -c: 字符串 只列出以字符串开头的 -u: 用户名 列出某个用户的 -p: pid 列出某个进程打开的文件 例子：lsof /sbin/init 查询某个文件被哪个进程调用lsof -c httpd 查询httpd进程调用了哪些文件lsof -u root 查询root的进程调用的文件名 系统定时任务可以是系统命令也可以是事先写好的脚本 一次性定时任务at at服务安装确认chkconfig --list |grep atd 是否安装at服务service atd restart at服务启动service atd status at服务的状态 at的访问控制/etc/at.allow文件中写入的用户才可以使用at命令（白名单）/etc/at.deny文件当白名单没有时适用，相当于黑名单（对root不起作用）如果系统没有这两个文件，默认只有root可以是用at命令 at命令at [option] 时间 -m at工作完成后，无论是否有命令输出，都用email通知执行at的用户 -c 工作号：显示at工作的实际内容 时间格式： HH:MM 02:30 HH:MM YYYY-MM-DD 02:30 2013-07-25 HH:MM [am|pm] [month] [date] 02:30 July 25 HH:MM [am|pm] + [minutes|hours|days|weeks] now + 5minutes 例子：at now + 2 minutes 在两分钟后执行脚本hello.shat &gt; /root/hello.sh &gt;&gt; /root/hello.log 在指定时间重启123at 02:00 2017-11-30&gt;/bin/sync &gt;/sbin/shutdown -r now /bin/sync 数据同步 1分钟后执行脚本hello.sh123456[root@VM_0_8_centos ~]# at now + 1 minutesat&gt; /root/hello.shat&gt; at&gt; &lt;EOT&gt;job 1 at 2017-11-26 17:43[root@VM_0_8_centos ~]# &lt;EOT&gt;输入ctrl+d退出 查询at工作atqat -c [工作号] 查看具体内容 删除at工作atrm [工作号] 循环定时任务crontab 确认crond服务是否安装和启动chkconfig crond onservice crond restart 访问控制/etc/at.allow文件中写入的用户才可以使用服务（白名单）/etc/at.deny文件当白名单没有时适用，相当于黑名单（对root不起作用）如果系统没有这两个文件，默认只有root可以是用服务 crontab设置crontab [option] -e: 编辑crontab定时任务 -l: 查询crontab任务 -r: 删除当前用户所有的crontab任务 例子执行crontab -e后会进入vim编辑界面[cron表达式] [执行的任务] 查询crontab -l 查询定时任务 删除所有crontab -r 清除所有定时任务 注意！！ 定时任务中的命令，应该使用绝对路径，因为定时任务的环境变量和我们的环境变量不一定一样 crontab配置crontab -e是不同身份的用户都可以执行自己的定时任务（只要在白名单中），可是有些定时任务需要系统执行，这里就需要/etc/crontab这个配置文件 123456789101112131415SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=rootHOME=/# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed SHELL: 使用的shell PATH: 定时任务的环境变量 MAILTO:如果定时任务出错，发邮件的对象 HOME:主目录位置 anacron 配置anacron是用来保证在系统关机的时候错过定时任务，可以在系统开机之后再执行判断漏掉的定时任务，找出来执行 anacron检测周期anacron会使用 一天， 七天，一个月来作为检测周期在系统的/var/spool/anacron/目录中存在cron.{daily,weekly,monthly}文件，用于记录上次执行cron的时间和当前时间作比较，若两个时间差超过了anacron的指定差值，证明又cron任务被漏掉执行 anacron配置文件/etc/anacrontab 1234567891011121314151617[root@VM_0_8_centos etc]# cat /etc/anacrontab# /etc/anacrontab: configuration file for anacron# See anacron(8) and anacrontab(5) for details.SHELL=/bin/shPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=root# the maximal random delay added to the base delay of the jobsRANDOM_DELAY=45# the jobs will be started during the following hours onlySTART_HOURS_RANGE=3-22#period in days delay in minutes job-identifier command1 5 cron.daily nice run-parts /etc/cron.daily7 25 cron.weekly nice run-parts /etc/cron.weekly@monthly 45 cron.monthly nice run-parts /etc/cron.monthly RANDOM_DELAY最大随机延迟；是随机执行时间不超过多少分钟，为的是，避免所有任务同时执行，例如重新开机，检测到漏掉的任务START_HOURS_RANGE执行时间在一定时间范围内执行1 5 cron.daily nice run-parts /etc/cron.daily如果发现上次执行时间和当前时间超过一台能，就执行/etc/cron.daily目录 5是强制延迟 nice run-parts 是执行的命令；一次类推，7天的，一个月的；7 25 cron.weekly nice run-parts /etc/cron.weekly@monthly 45 cron.monthly nice run-parts /etc/cron.monthly run-parts 是一个脚本，脚本和二进制命令都可以被调用，知识脚本慢一点；这个脚本大概作用就是 取出命令后的目录里面所有的脚本执行 anacron例子以下用 cron.daily工作来说明执行的过程 首先读取/var/spool/anacron/cron.daily中上一次anacron执行的时间 和当前时间比较，如果两个时间的差值超过一天，就执行cron.daily工作 执行这个工作只能在 03:00-22:00(START_HOURS_RANGE=3-22) 之间 执行工作时强制延迟时间为 5分钟 (1 5 cron.daily nice run-parts /etc/cron.daily)，再随机延迟 0-45分钟(RANDOM_DELAY=45) 使用nice命令(1 5 cron.daily nice run-parts /etc/cron.daily)指定默认优先级，使用run-parts脚本执行/etc/cron.daily目录中的所有可执行文件]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux软件安装管理]]></title>
    <url>%2F2018%2F11%2F24%2FLinux%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[软件包管理软件包分类源码包linux 一开始的包就是 .tar.gz 的源码包，打开可以看到里面是纯C语言的源码包优点和缺点： 开源 可以自由选择功能 软件是编译安装，所以更加适合自己的系统，更加稳定，效率更高 卸载方便 安装过程步骤多，尤其装较大软件集合的时候 编译过程时间较长，比二进制长 rpm包也叫 二进制 包，系统默认包是将源码包经过编译后的二进制包windows中的软件包是经过编译的，是二进制包优点和缺点： 包管理系统简单，安装，升级，查询，卸载简单 安装速度比源码包快 经过编译，不能看到源码 功能依赖不如源码包灵活 依赖性，一个包依赖另一个包，需要取装另一个包，另一个包又依赖其他几个包等 源码包和rpm包选择？因为rpm包是别人已经编译好的，他不一定最适合我的机器，所以，对于大型服务，有很多访问的最好使用源码包 脚本安装包其实linux中就rpm包和源码包两种，脚本安装包就是把复杂的安装过程写成程序脚本，实际安装的还是源码包或二进制包 rpm命令管理rpm包的来源如果通过光盘来的，挂载/mnt/cdrom/Packages rpm包命名规则软件包名-软件版本-软件发布次数.适合的linux平台.适合的硬件平台.rpm例如httpd-2.2.15-15.el6.centos.1.i686.rpmjdk-7u80-linux-x64.rpm rpm包的依赖性包名和包全名的概念httpd-2.2.15-15.el6.centos.1.i686.rpm是包全名，其中的httpd是包名包全名：操作的包是没有安装的软件包时，使用包全名包名：操作已经安装的软件包时，使用包名，是搜索/var/lib/rpm/中的数据库 rpm安装rpm -ivh package_full_name.rpm-i 安装-v 显示详细信息-h 显示进度 升级与卸载rpm -Uvh package_full_name.rpm-U 升级rpm -e package_name-e 卸载其实卸载也可以手动去删除相关的文件就行，只是通过rpm包装的文件会分布在各个地方，使用命令能方便删 rpm查询查询是否安装rpm -q 包名 查询已经安装的所有rpm包rpm -qa可用管道符通过grep去查 查询软件包的详细信息rpm -qi 包名 rpm包默认安装位置 位置 描述 /etc/ 配置文件安装目录 /usr/bin/ 可执行命令安装目录 /usr/lib/ 程序使用的函数库保存位置 /usr/share/doc/ 基本软件使用手册保存位置 /usr/share/man/ 帮助文件保存位置 查询包中文件安装位置rpm -ql 包名 查询系统文件属于哪个RPM包rpm -qf 系统文件名 查询软件包的依赖性rpm -qR 包名-R 查询软件包的依赖性-p 查询未安装包的信息 RPM包校验rpm -V 包名可以返回这个包中哪些文件被人修改过，即便是注释改变也一样会报例如S.5....T. c /etc/httpd/conf/httpd.conf验证信息对应表：| S | 文件大小是否改变 || —- | ————————- || M | 文件类型或者文件权限（rwx）是否改变 || 5 | 文件MD5校验和是否改变（相当于文件内容是否改变） || D | 设备主从代码是否改变 || L | 文件路径是否改变 || U | 文件所有者是否改变 || G | 文件所属组是否改变 || T | 文件修改时间是否改变 || . | 没有改变 |c 代表文件类型；对应表| c | 配置文件(config file) || —- | ————————- || d | 普通文件(documentation) || g | ghost file 鬼文件，表示不该被rpm包含 || r | 描述文件(read me) || L | 授权文件 (license file) | rpm包中文件提取rpm2cpio 包全名 | cpio -idv.文件绝对路径-rpm2cpio 将rpm包转换为cpio格式的命令-cpio 一个标准工具一般用于，不小心删除了某个命令；从别的包提取过来例如 现在不小心删除了 /bin/ls 命令rpm -qf /bin/ls先查询文件属于哪个包rpm2cpio /mnt/cdrom/Packages/coreutils-8.4-19.el6.i686.rpm | cpio -idv ./bin/ls 提取RPM包中的ls命令到当前目录的/bin/ls 下cp /root/bin/ls /bin/ 把ls文件复制回去，修复文件丢失 yum在线管理官方提供服务器，将所有软件包都放到官方服务器上，当进行yum在线安装的时候，可以自动解决依赖性问题 yum源文件位置：/etc/yum.repos.d/文件：CentOS-Base.repo 12345678[base]name=CentOS-$releasever - Base - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/os/$basearch/ http://mirrors.aliyuncs.com/centos/$releasever/os/$basearch/ http://mirrors.cloud.aliyuncs.com/centos/$releasever/os/$basearch/gpgcheck=1gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-6 [base] 容器名称 name 容器说明，可以自己随便写 mirrorlist 镜像站点 baseurl yum源服务器地址，默认是CentOS官方yum源服务器 enabled 次容器是否生效，不写默认生效 ；1：生效；0：不生效 gpgcheck 1 指rpm数字证书生效，0 不生效 gpgkey 数字证书公钥文件保存位置 光盘搭建yum源yum源默认是用网络来作为yum源；如果不想通过网络，如何使用yum源？可以搭建本地yum源，使用光盘等mkdir /mnt/cdrom/ 建立挂载点mount /dev/cdrom /mnt/cdrom/ 挂载光盘cd /etc/yum.repos.d 进入yum源目录mv CentOS-Base.repo CentOS-Base.repo.bak 是在线yum源失效，模拟，或者改文件内容 enabled = 0vi CentOS-Media.repo 修改光盘的yum源，使其生效接下来将baseurl 项改为baseurl = file:///mnt/cdrom 改为光盘挂载点地址enabled = 1 把enabled 改为1 使其生效 yum命令查询yum list 查询所有可用软件包列表yum search 关键字 搜索服务器上所有关键字相关的包 安装yum -y install 包名-y自动回答yes 升级yum -y update 包名-y 自动回答yes 卸载yum -y remove 包名 yum软件组管理命令yum grouplist 列出所有可用的软件组列表yum groupinstall &quot;软件组名&quot; 安装指定软件组，软件组名可有yum grouplist查出yum groupremove &quot;软件组名&quot; 卸载指定软件组 源码包管理源码包和rpm包区别安装之前的区别：概念上的区别安装之后的区别：安装位置不同 源码包安装位置任意，但是一般我们把它放在/usr/local/中源码包没有卸载命令，需要找到文件位置，手动删除 源码包安装 安装C语言编译器—gcc 下载源码包—官方网站下载 解压缩下载的源码包tar 进入源码包cd 执行./configure类似的构建文件配置与检查，可以通过./configure --help 查看帮助，定义需要的功能选项，检测环境是否符合安装要求；把定义好的功能选项和检测系统环境信息都写入Makefile文件，便于后续的编辑 指定安装位置例如 ./configure --prefix=/usr/local/apache2 make编译 –直接在当前目录下make make install 源码包的INSTALL一般会有安装说明 源码包卸载直接删除安装的位置目录文件即可 安装注意事项源代码保存位置：/usr/local/src/软件安装位置：/usr/local/ make编译make clean 万一报错，可以使用这个命令清除make install 编译安装 如果编译安装出错，不仅要make clean，还要删除/usr/local/下的文件夹 脚本安装包]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[胡思乱想]]></title>
    <url>%2F2018%2F11%2F24%2F%E7%96%91%E9%97%AE%2F</url>
    <content type="text"><![CDATA[Setting Account Resource Limits今天在看mysql的官方文档的时候，看到这个限制用户每小时的查询数量，联想到一个问题，这个是不是可以用作一些基础服务，如user之类的，user库可能会对多个业务提供基础服务，但是，万一有一个业务疯狂查询把基础服务搞挂了，那会影响到其他的业务，所以，这个是否能用来限制那个可能会把user搞挂的业务的查询数量，哪怕自己业务出延迟，也不要把基础服务搞挂影响到其他业务 mysql官方文档：https://dev.mysql.com/doc/refman/5.7/en/user-account-management.html —-以上是小白的胡思乱想。 数据库备份1.shell脚本2.定时任务3.mysqldump]]></content>
      <categories>
        <category>idea</category>
      </categories>
      <tags>
        <tag>服务器高并发预防</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux网络管理]]></title>
    <url>%2F2018%2F11%2F24%2FLinux%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[linux网络配置linux配置IP地址ifconfig命令临时配置IP地址ifconfig:查看与配置网络状态命令；相当于windows中的 ipconfigifconfig eth0 192.168.0.200 netmask 255.255.255.0–临时设置eth0网卡的ip地址和子网掩码 12345678910111213141516[root@VM_0_8_centos ~]# ifconfigeth0 Link encap:Ethernet HWaddr 52:54:00:94:51:EE inet addr:172.16.0.8 Bcast:172.16.15.255 Mask:255.255.240.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:4720653 errors:0 dropped:0 overruns:0 frame:0 TX packets:4873615 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:3306184967 (3.0 GiB) TX bytes:3313174495 (3.0 GiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:413 errors:0 dropped:0 overruns:0 frame:0 TX packets:413 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:22923 (22.3 KiB) TX bytes:22923 (22.3 KiB) 这里显示的两块eth0和lo是linux中的两块网卡lo是每个服务器都有的，没有实际使用，只用来表示，当前协议是正常的Link encap:Ethernet当前网络类型是以太网HWaddr 52:54:00:94:51:EE网卡物理地址MACinet addr:172.16.0.8 Bcast:172.16.15.255 Mask:255.255.240.0 当前ip地址，广播地址，子网掩码UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 当前网卡参数RX packets:4720653 errors:0 dropped:0 overruns:0 frame:0 当前接收到的数据包TX packets:4873615 errors:0 dropped:0 overruns:0 carrier:0当前发送的数据包RX bytes:3306184967 (3.0 GiB) TX bytes:3313174495 (3.0 GiB)接收数据包总大小和发送数据包的总大小 setup工具永久配置IP地址setup是红帽专有的图形化工具，不用setup也可以使用修改linux网络配置文件service network restart使修改生效 网络配置文件网卡信息文件/etc/sysconfig/network-scripts/ifcfg-eth0内容如下 1234567891011[root@VM_0_8_centos ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0# Created by cloud-init on instance boot automatically, do not edit.#BOOTPROTO=dhcpDEVICE=eth0HWADDR=52:54:00:94:51:eeNM_CONTROLLED=noONBOOT=yesTYPE=EthernetUSERCTL=noPERSISTENT_DHCLIENT=yes 对于网卡信息的配置通常包括：配置IP地址、子网掩码和网关。网卡信息保存在网卡配置文件中。网卡配置文件位于/etc/sysconfig/network-scripts目录下。一块网卡对应一个网卡配置文件，配置文件命名规则：ifcfg-网卡类型以及网卡的序列号由于以太网卡类型是eth，网卡的序列号从0开始，所以第一块网卡的配置文件名称为ifcfg-eth0，第二块网卡为ifcfg-eth1，以此类推。网卡配置文件中常用配置文件名的还以如下：|参数项 |作用描述||————————– |–||DEVICE=eth0 |定义该网卡的识别名称||BOOTPROTO=dhcp |是否自动获取IP（none，static，dhcp）static/none：代表固定的IP地址；bootp/dhcp：通过BOOTP或DHCP协议取得IP地址||HWADDR=52:54:00:94:51:ee |该网卡的MAC地址||HWADDR=52:54:00:94:51:ee |该网卡的MAC地址||ONBOOT=yes|启动network服务时，是否启用该网卡。当RedHat系统启动network服务时，network服务一次读取保存于/etc/sysconfig/network-scripts/目录下所有网卡的配置文件。如果网卡配置文件的ONBOOT设置为yes，则network服务就会调用ifup命令启动该网卡；如果网卡的配置文件的ONBOOT参数为no，network会跳过启动这个网卡的工作。||TYPE=Ethernet |网卡的类型，以太网||USERCTL=no |是否允许普通用户启动或者停止该网卡，一般设置no只能root操作||IPV6INIT=no |是否在该网卡上启动IPV6的功能||PEERDNS=yes |是否允许网卡在启动时向DHCP服务器查询DNS信息，并自动覆盖/etc/resolv.conf配置文件||IPADDR=192.168.1.55 |静态方式指定网卡的IP地址，置项用于指定该网卡的静态IP地址，此时BOOTPROTO必须为static或者none||NETMASK=255.255.255.0 |定义该网卡的子网掩码||GATEWAY=192.168.1.1 |设置网络的默认网关||DNS1=192.168.128.5 |指定主要的DNS服务器地址||DNS2=192.168.128.6 |指定备用的DNS服务器地址||UUID= |唯一识别码||MTU=1500 |设置网卡的MAC帧最大传输单位大小| BOOTPROTO=dhcp 如果局域网内没有DHCP服务器，则无法使用 更多参考：https://www.cnblogs.com/wolfkingzzy/archive/2012/04/07/Linux%E7%BD%91%E5%8D%A1%E9%85%8D%E7%BD%AE.htmlhttps://www.imooc.com/video/5223 主机名文件/etc/sysconfig/network 12345[root@VM_0_8_centos ~]# cat /etc/sysconfig/network# Created by cloud-init on instance boot automatically, do not edit.#NETWORKING=yesHOSTNAME=VM_0_8_centos NETWORKING必须为yes，为no，网络无法工作HOSTNAME为主机名，在windows中相同局域网下不能有同样的主机名，而linux中无所谓； DNS配置文件/etc/resolv.conf nameserver 名称服务器，如果有多个，可以空格接上其他的 linux网络命令网络环境查看命令ifconfig关闭和启动网卡 ifdown 网卡设备名（eth0，ol等） –禁用该网卡设备 ifup 网卡设备名 –启用该网卡设备 查询网络状态netstatnetstat [option]-t 列出TCP协议端口-u 列出UDP协议端口-n 不使用域名与服务器名，而使用ip地址和端口号-l 仅列出在监听状态的网络服务-a 列出所有网络连接-rn 相当于 route -n netstat -tuln看到在监听对应的端口号知道了对应服务开启了 netstat -an![img]可以看到自己写的一些程序 想查看ESTABLISHED连接数netstat -an |grep ESTABLISHED | wc -l这里wc -l 看行数 各项解释1、0.0.0.0代表本机上可用的任意地址。 比如0.0.0.0:135 表示本机上所有地址的135端口，这样多ip计算机就不用重复显示了。2、TCP 0.0.0.0:80表示在所有的可用接口上监听TCP80端口3、0.0.0.0为默认路由，即要到达不再路由表里面的网段的包都走0.0.0.0这条规则然后127.0.0.1就是表示本机ip地址的意思了。[::]:21表示ipv6的21号端口的意思。还有UDP的外部链接怎么都是:呢？：是网址的通配符，就是192.168.15.12，这个类型的整体描述。 各状态意义表 statu | description :—: | :—: LISTEN | (Listening for a connection.)侦听TCP端口的连接请求 SYN-SENT | (Active; sent SYN. Waiting for a matching connection request after having sent a connection request.)发送连接请求后等待匹配的连接请求 SYN-RECEIVED| (Sent and received SYN. Waiting for a confirming connection request acknowledgment after having both received and sent connection requests.)收到和发送一个连接请求后等待对方对连接请求的确认 ESTABLISHED | (Connection established.)代表一个打开的连接 FIN-WAIT-1 | (Closed; sent FIN.)等待远程TCP连接中断请求，或先前的连接中断请求的确认 FIN-WAIT-2 | (Closed; FIN is acknowledged; awaiting FIN.)从远程TCP等待连接中断请求 CLOSE-WAIT | (Received FIN; waiting to receive CLOSE.)等待从本地用户发来的连接中断请求 CLOSING | (Closed; exchanged FIN; waiting for FIN.)等待远程TCP对连接中断的确认 LAST-ACK | (Received FIN and CLOSE; waiting for FIN ACK.)等待原来的发向远程TCP的连接中断请求的确认 TIME-WAIT | (In 2 MSL (twice the maximum segment length) quiet wait after close. )等待足够的时间以确保远程TCP接收到连接中断请求的确认 CLOSED | (Connection is closed.)没有任何连接状态 routeroute [option]-n 查看路由列表 nslookupnslookup 主机名或者ip进行域名或者ip地址解析 12345678910111213[root@VM_0_8_centos ~]# nslookup dosncoco.github.ioServer: 183.60.83.19Address: 183.60.83.19#53Non-authoritative answer:Name: dosncoco.github.ioAddress: 185.199.109.153Name: dosncoco.github.ioAddress: 185.199.108.153Name: dosncoco.github.ioAddress: 185.199.110.153Name: dosncoco.github.ioAddress: 185.199.111.153 ServerDNS服务器地址 网络测试命令pingping [option] ip或域名-c ping次数探测指定ip或者域名的网络状况 telnettelnet [域名或ip] [端口]远程管理与端口探测命令不加密，相对于的加密：ssh traceroutetraceroute [option] 域名或ip-n 使用ip，不使用域名，速度更快可以追踪路由经过哪里，一般用来路由故障定位（不通） wgetwget url下载命令 tcpdumptcpdump -i eth0 -nnX port 21-i 指定网卡接口-nn 将数据包中的域名与服务转为IP和端口-X 以十六进制和ASCII码显示数据包内容port 指定监听端口 更多：https://www.imooc.com/video/5454 远程登录SSH 协议原理对称加密算法采用单钥密码系统的加密方法，同一个密钥可以同时用作信息的加密和解密，这种加密方法称为对称加密，也称为单密钥加密 非对称加密算法又名公开密钥加密算法，需要两个密钥，公开密钥和私有密钥 安全外壳协议sshssh是由非对称加密演变而来的ssh username@ip_addrexit退出scp [-r] 用户名@ip:文件路径 本地路径 下载文件 如果是目录要加-rscp [-r] 本地文件 用户名@ip:上传路径 上传文件 如果是目录要加-r 1234[root@VM_0_8_centos ~]# ssh donscoco@47.93.233.184The authenticity of host '47.93.233.184 (47.93.233.184)' can't be established.RSA key fingerprint is 2b:97:15:e1:fa:3c:14:fa:9b:48:48:62:0e:32:d7:6d.Are you sure you want to continue connecting (yes/no)? 会询问是否下载对方的公钥下载的公钥会放在家目录下的 .ssh 中的 known_hosts这里如果对方重新生成公钥或者计算机重装系统之类的。原来的下载在我们这边的公钥会无法使用，只需要删除即可，下次连接的时候又会询问我们是否下载公钥]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[团队代码规范]]></title>
    <url>%2F2018%2F11%2F24%2F%E5%9B%A2%E9%98%9F%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[前言一个项目大多都是由一个团队来完成，如果没有统一的代码规范，那么每个人的代码必定会风格迥异。规范的代码: 可以促进团队合作 可以减少bug处理 可以降低维护成本 有助于代码审查等等众多好处 所以今天打算用在YY实习期间，了解到的YY团队的代码规范，结合阿里团队JAVA的开发规范学习一下以下内容来自云周刊。 规范插件安装插件是什么？IDEA插件安装相关说明IDEA版的插件已发布到IDEA官方仓库中(最低支持版本14.1.7，JDK1.7+)，只需打开 Settings &gt;&gt; Plugins &gt;&gt; Browse repositories 输入 Alibaba 搜索一下便可以看到对应插件了，点击安装等待安装完成。至于如何使用请大家到官方Github仓库中进行查看。IDEA会自动检测插件新版，并提示出来，所以大家不用担心插件的更新问题。 Eclipse插件安装相关Eclipse版插件支持4.2（Juno，JDK1.8+）及以上版本，我们提供自主的Update Site，通过 Help &gt;&gt; Install New Software 然后输入https://p3c.alibaba.com/plugin/eclipse/update 即可看到安装列表。大家可以通过 Help &gt;&gt; Check for Udates 进行插件新版检测。 插件的安装阿里巴巴Java开发手册IDEA插件使用指南通过Jetbrains官方仓库安装 打开 Settings &gt;&gt; Plugins &gt;&gt; Browse repositories… 在搜索框输入alibaba即可看到Alibaba Java Code Guidelines插件，点击Install进行安装，然后重启IDE生效 注意：因为插件zip包托管在Jetbrains官方CDN上，所以是从国外的服务器进行下载，可能会出现超时的情况 通过下载安装包进行安装如果不能通过Jetbrains官方仓库安装，那也可以使用这样的方法 打开插件页面https://plugins.jetbrains.com/plugin/10046-alibaba-java-coding-guidelines下载 Settings &gt;&gt; Plugins &gt;&gt; Install plugin from disk…，选择刚刚下载的zip包安装，然后重启IDE注意最低支持IDEA版本为14.1（buildNumber 141.0，可以在About Intellij IDEA中查看版本信息），使用IDEA14的同学最好升级到14.1.7 阿里巴巴Java开发手册Eclipse插件使用指南篇幅原因这里就不做介绍了，详细可登陆阿里云周刊查看https://yq.aliyun.com/articles/224817 中文乱码解决方法 修改字体——Appearance&amp;Behavior -&gt; Appearance -&gt; UI Options -&gt; Name 里面设置成微软雅黑（microsoft yahei light） Switch Language to English and restart. 插件的使用目前插件实现了集团编码规约中的41条规则，大部分基于PMD实现，其中有4条规则基于IDEA实现，并且基于IDEA Inspection实现了实时检测功能。部分规则实现了Quick Fix功能，对于可以提供Quick Fix但没有提供的，我们会尽快实现，也欢迎有兴趣的同学加入进来一起努力。目前插件检测有两种模式：实时检测、手动触发。 实时检测实时检测功能会在开发过程中对当前文件进行检测，并以高亮的形式提示出来，同时也可以支持Quick Fix，该功能默认开启，可以通过配置关闭。结果高亮提示检测结果高亮提示，并且鼠标放上去会弹出提示信息。 QuickFix功能Alt+Enter键可呼出Intention菜单，不同的规则会提示不同信息的Quick Fix按钮 关闭实时监测在某些情况下，我们不希望对代码提示违规信息，比如我们在阅读Github开源项目代码的时候，如果界面出现一堆红色、黄色的提示，此时心里肯定是飘过一万只草泥马。这个时候我们可以通过Inspection的设置关闭实时检测功能。 通过右键快速关闭（打开）所有规则的实时检测功能 通过Settings &gt;&gt; Editor &gt;&gt; Inspections 进行手动设置 也可以关闭某条规则的实时检测功能或者修改提示级别。 代码扫描可以通过右键菜单、Toolbar按钮、快捷键三种方式手动触发代码检测。同时结果面板中可以对部分实现了QuickFix功能的规则进行快速修复。 触发扫描：在当前编辑的文件中点击右键，可以在弹出的菜单中触发对该文件的检测。在左侧的Project目录树种点击右键，可以触发对整个工程或者选择的某个目录、文件进行检测。如果您打开了IDE的Toolbar，也可以通过Toolbar中的按钮来触发检测，目前Toolbar的按钮触发的检测范围与您IDE当时的焦点有关，如当前编辑的文件或者是Project目录树选中的项，是不是感觉与右键菜单的检测范围类似呢。使用快捷键（Ctrl+Shift+Alt+J）触发弹出窗口，选择检测范围；您也可自定义快捷键。 扫描结果：检测结果直接使用IDEA Run Inspection By Name功能的结果界面，插件的检测结果分级为Blocker、Critical、Major。默认按等级分组，方便统计每个级别错误的数量。默认情况我们在结果面板需要双击具体违规项才能打开对应的源文件，开启Autoscroll To Source选项，单击面板中的文件名、或者是具体的违规项的时候IDEA会自动打开对应的源文件。 QuickFix：对于实现Quick Fix的规则，在结果面板中可以直接一键修复 注意：IDEA14、15可以通过左下角的灯泡进行一键修复操作。 更多参考：https://yq.aliyun.com/articles/224817 阿里巴巴Java开发手册 - GitHub：https://github.com/alibaba/p3c《阿里巴巴Java开发手册》IDEA插件与Eclipse插件使用指南https://yq.aliyun.com/articles/224817?spm=a2c4e.11153940.blogcont224345.14.60ce1178tCbuwi提升团队研发效能利器，《阿里巴巴Java开发手册》插件全球首发（附插件下载地址）https://yq.aliyun.com/articles/224345?spm=a2c4e.11153940.blogcont224817.9.7df43b97RRVL2w 团队代码规范请使用 intellij idea 作为日常的代码开发IDE，在 plugins 里安装 Alibaba Java Coding Guidelines。团队内部将全面使用 Ali CodeStyle 进行规范代码！ Java（服务端、Android） 导入YY-CodeStyles.xml(https://github.com/donscoco/codeSytle/blob/master/YY-CodeStyles.xml) 导入YY-Inspections.xml(https://github.com/donscoco/codeSytle/blob/master/YY-Inspections.xml) 配置File Header 123456/** * * @author $&#123;USER&#125; * @version 1.0.0 * @since 1.0.0 */ Inspections注意：需要选择 Default IDE，级别的，设置为全局的，而非project级别的！]]></content>
      <tags>
        <tag>code style</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux权限管理]]></title>
    <url>%2F2018%2F11%2F24%2Flinux%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[前言这里只是在学习linux相关知识过程中，对常用命令做的一个总结笔记更多linux相关知识可访问 http://linux.vbird.org/ 学习 文件基本权限 常见文件类型有7种，除了上面3种，还有套接字文件，管道符文件系统文件等 基本权限的作用读r 写w 执行x 关于目录和文件的权限之间的关系，联系文件在硬件中的分区，区块来思考；例如，我对一个文件没有任何权限，但是我对当前文件夹有读写权限，我可以看到这个没有权限的文件，因为我对这个文件所属的块没权限，但是这个权限是“放在”我的文件夹块中，我可以看到自己文件这一块的信息 把文件夹当做为文件，去理解权限 权限对文件的作用r：读写文件的内容w: 编辑新增修改，但是不能删除x: 可以执行 权限对目录的作用r: 可以查询目录下的文件名 (ls)w: 可以修改目录的结构，比如新建文件和目录，删除文件或目录，重命名文件或文件夹(touch rm mv cp)x: 可以进入目录(cd) 基本权限的修改chmod [option] 模式 文件名-R 递归 就是说文件夹下的所有文件都赋予 模式写法1例如 chmod u+x learning_temp_file给文件的所有者u(所属组g,其他人o)添加上执行权限x(读r,写w)例如 chmod u+x,g+x learning_temp_file多个赋予可以用,分隔例如 chmod u-x learning_temp_file可以+权限，相应地也就可以-权限 写法2例如 chmod u=rwx,g=rw learning_temp_file 写法3使用数字来代表权限r—4w—2x—1例如 rwxr-xr-x 就可以表示为 755chmod u=rwx,g=rx,o=rx learning_temp_file 相当于 chmod 755 learning_temp_file一般对目录的权限赋予 0 5 7 用户组权限修改在创建用户的时候如果不指定组就会创建一个和用户名相同的组 chown 用户名 文件名chown donscoco:donscoco_group file_name 也可以直接添加组 修改所有组chgrp 组名 文件名 小思考任务现在我需要为一个老师创建一个目录老师是目录的所有者，拥有所有权限老师的学生拥有查看这个目录的权限其他人没有这个目录的权限 处理groupadd teacher_group 创建一个老师的分组useradd -G teacher_group teacher 为分组添加用户useradd -G teacher_group student1mkdir teacher_course_dir为创建一个老师的文件夹chmod 750 teacher_course_dir 给文件赋予不同角色对应的权限chown teacher:teacher_group teacher_course_dir 将文件的所有权交给老师,假设老师的分组为teacher_group 文件默认权限umask查看默认权限0022第一位0：文件特殊权限022：文件默认权限可以 在umask后面添加值 例如 umask 026,但是这是临时的 目录默认权限目录默认权限为777默认权限777-umask的022 为 755 文件默认权限文件默认不能建立执行文件，必须手工赋予执行权限所以文件默认权限最大为666默认权限需要换算成字母在相减建立后的默认权限，为666-umask的022 修改unmask###临时修改1umask 026 ###永久修改1vi /etc/profile 文件特殊权限ACL权限文件都只有 一个所有者，一个用户组，还有其他三种角色,如果出现：这里ACL出现就是为了解决用户身份不够的情况的 查看分区的ACL权限是否开启dumpe2fs -h 目标分区 –目标分区如/dev/sda5 等；可通过df或者mount查看文件系统(对应分区)的挂载点-h 显示超级块的信息，而不显示磁盘块组的详细信息在返回的信息中如果有这个Default mount options: user_xattr acl就表示分区可以使用acl权限 开启分区ACL权限临时开启重新挂载根分区并挂载加入ACL权限mount -o remount,acl / 永久开启修改配置文件vi /etc/fstab在配置文件中加入UUID=1ae5f12f-79c1-47d4-bed7-941ab9385396 / ext4 defaults,acl 1 1然后重启系统或者重新挂载文件系统mount -o remount /ps: /etc/fstab 在改动时要非常小心，这个文件会直接影响linux系统的启动 ACL权限查看getfacl 文件名 ACL权限设定与删除setfacl [option] 文件名[option]-m：设定ACL权限-x：删除指定的ACL权限-b：删除所有的ACL权限-d：设定默认的ACL权限-k：删除默认的ACL权限-R：递归设定ACL权限 ACL权限设定给用户dons设定对dir_name的读写权限setfacl -m u:dons:rw dir_name给用户组dons_group设定对dir_name的读写权限setfacl -m g:dons_group:rw dir_name这个+代表就是ACL权限，具体什么样的权限就需要通过getfacl 文件名 来查看 ACL权限删除给用户dons删除对dir_name的权限setfacl -x u:dons dir_name给用户组dons_group设定对dir_name的读写权限setfacl -x g:dons_group dir_name删除文件的所有ACL权限setfacl -b dir_name ACL权限递归给 dir_name 下的 其他文件赋予acl权限setfacl -m u:dons:rx -R dir_name递归是父目录在设定ACL权限时，所有的子文件和子目录也会拥有相同的ACL权限 ACL默认权限在对一个文件夹执行赋予递归ACL权限之后，默认在这个文件夹新建的文件是没有ACL权限的ACL默认权限：默认权限的作用是如果给父目录设定了默认的ACL权限，那么父目录中所有新建的子文件都会继承父目录的ACL权限setfacl -m d:u:dons:rx -R dir_name 最大有效权限maskmask是用来指定最大有效权限的。如果我给用户赋予ACL权限，是需要和mask的权限做与运算才能得到用户真正的权限的 修改最大有效权限setfacl -m m:rx 文件名设定mask权限为r-x 使用 m:权限 格式 sudo权限什么是sudo权限？root 把本来只能超级用户执行的命令赋予普通用户执行sudo 操作对象是系统命令；之前将的权限都是用户操作文件的权限； visudo 命令打开/etc/sudoers文件 其中有一条root ALL=(ALL) ALL中root 表示给哪个用户赋予sudo权限ALL=(ALL) 被管理者的主机地址 = （可使用的身份）;表示 允许 root 在任何主机中使用任何身份ALL 授权命令（绝对路径）；比如说如果想给一个用户donscoco赋予重启权限可以在配置文件/etc/sudoers中加入donscoco ALL=(ALL) /sbin/shutdown -r now ps：可以通过 man 5 /etc/sudoers 来查看配置文件的帮助说明 sudo的安全问题给其他用户赋予能创建新用户的权限donscoco ALL=/usr/sbin/useradd没有括号（ALL）默认就是按root身份;可以切换到donscoco使用sudo -l查看 这里在赋予设置密码权限的时候，千万不能直接使用/usr/bin/passwd因为使用sudo系统是把用户donscoco当做root的身份来对待的，用户donscoco可以使用 sudo /usr/bin/passwd root 直接更改root 的 密码使用密码赋予权限：donscoco ALL=/usr/bin/passwd [A-Za-z]*, !/usr/bin/passwd &quot;&quot;, !/usr/bin/passwd root!/usr/bin/passwd root 禁止 使用/usr/bin/passwd命令对root进行操作 给其他用户赋予vi命令/etc/shadow 这个文件是保存密码的文件任何用户对这个权限都是000,包括root但是root 可以查看和更改/etc/shadow的内容如果我们在给一个用户赋予 /bin/vi 的权限这个用户使用sudo /bin/vi /etc/shadow 去改变密码或者删除密码，那么也是一个很严重的问题 其他安全相关的权限这里以下的权限尽量少修改 SetUIDSetUID功能只有可执行的二进制程序才能设定SUID权限命令执行者要对该程序有x可执行权限命令执行者在执行该程序时获得该程序文件属主的身份SetUID只在该程序执行过程中有效，也就是说身份改变只在程序执行过程中有效 /etc/shadow文件权限是000，那么普通用户在改自己密码的时候又是怎么对 /etc/shadow进行修改的呢？这里 /usr/bin/passwd的信息如下passwd 命令被设置了SUID权限只要执行/usr/bin/passwd命令，执行命令人的身份就会变成执行文件所有者的身份root，所以普通用户可以改自己的密码 设定SUIDchmod 4775 file_namechmod u+s file_name这里4 就是SUID的意思，也可以使用第二句 取消SUIDchmod 0775 file_namechmod u-s file_name 定期检查危险的SUID123456789101112131415161718#!/bin/bashfind / -perm -4000 -o -perm -2000 &gt; /tmp/setuid.check#搜索系统中所有拥有SUID和SGID的文件，并保存到临时文件中for i in $(cat /tmp/setuid.check)#做循环，每次循环取出临时文件中的文件名do grep $i /root/suid.log &gt; /dev/null #对比这个文件名是否在模板文件中 if["$?"!="0"] #检查上一个命令的返回值，如果不为0，证明上一个命令报错 then ehco "$i is not in listfile!" &gt;&gt; /root/suid_log_$(date+%F) #如果文件名不在模板中，则输出错误信息，报错到日志中 fidonerm -rf /tmp/setuid.check#删除临时文件 SetGID只有可执行的二进制程序才能设置SGID权限命令执行者要对该程序有x执行权限命令执行在执行程序的时候，组身份升级wei为该程序文件的属组SetGID权限同样只在该程序执行过程中有效，也就是说组身份改变只在程序执行过程中有效 Sicky BITchattr权限chattr [+-=] [选项] 文件或目录名-+：增加权限–：删除权限-=：等于某权限]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP protocol]]></title>
    <url>%2F2018%2F10%2F25%2FHTTP%2F</url>
    <content type="text"><![CDATA[名词解析 URI :(uniform resource identifier) 统一资源定位符；用来标识互联网上的唯一资源 URL :(uniform resource locator) 统一资源定位器；e.g:http://user:pass@host.com:80/path?query=string#hash 此类格式都叫URL，http，ftp协议等 URN :(Uniform Resource Name) 统一资源名称 TCP协议 HTTP请求返回的完整过程： 其中每个节点都代表HTTP为我们做的事情 网络模型网络模型有七层标准模型和经典五层模型，这里展示五层模型 物理层：定义物理设备如何传输数据 数据链路层：通信实体间建立数据链路连接 网络层：为数据在实体间传输创建逻辑链路 传输层：向用户提供可靠的端到端(end to end)的服务传输层主要有两个协议TCP和UDP；什么是端到端的服务呢？比如我们建立了客户端到服务器的连接之后，他们两端如何去传输数据，传输的方式都是在这里定义的；我们传输的数据有可能很小有可能很大，如果很大不能一次传输过去，那么就要分包分片；分后传输过去后又如何组装也是在这里定义的； 应用层：为软件提供服务例如HTTP；构建于TCP协议之上的；屏蔽了网络传输相关细节 HTTP报文格式 http报文都有一个首行（不属于请求头） method：GET POST DELET PUT … URL： 协议版本 返回码code: 不同code代表不同的意思 200,,301，302，404,500等 header 首行换行后就是header body header 换行后再空行 就是body 认识HTTP Client 什么是HTTP Client？只要实现了一个发送标准HTTP请求的工具，那就是HTTP Clientps: http只是负责返回数据，而真正展示页面的是浏览器，浏览器拿到数据后对数据进行渲染，得到结果展示给用户 使用 curl 工具，在命令行中 curl baidu.com会得到为什么和在浏览器看到的不一样？因为浏览器在得到数据解析识别 meta 后，根据 http-equiv 重定向到 www.baidu.com 浏览器同域限制 跨域如何请求资源？被请求资源的http头设置 ‘Access-Control-Allow-Origin’:’*’; 也可以设置具体的url地址，指定特定的服务能请求e.g：’Access-Control-Allow-Origin’:’http://donscoco.github.io&#39;---只有http://donscoco.github.io可以跨域请求这个资源ps:就算头没有设置，请求也是能请求到，只是浏览器识别不是同一域而且没有’Access-Control-Allow-Origin’所以会忽略 什么是跨域？跨域，指的是浏览器不能执行其他网站的脚本。 它是由浏览器的同源策略造成的，是浏览器施加的安全限制。浏览器运行标签上的路径加载内容，不会在乎是否加了’Access-Control-Allow-Origin’:’*’; CORS预请求 缓存 可缓存性Cache-Control:http请求所经过的路径中，包括经过的一些http代理服务器都可以对http的内容进行缓存的操作public 浏览器，代理服务器都进行缓存private 只有浏览器缓存no-cache 不缓存 到期max-age=设置缓存时间；过期之后浏览器才会再次发送请求到服务端s-maxage=代理服务器读取的缓存时间 重新验证must-revalidate 在max-age 过期后用于验证proxy-revalidate 用于缓存服务器的过期验证 其他no-storeno-transform 验证头Last-Modified 上次修改时间Etag数据签名:常用的数据签名是对内容进行一个哈希计算；配合if-Match或者if-Non-Match使用对比资源签名判断是否使用缓存; Cookie cookie通过Set-Cookie设置 e.g: ‘Set-Cookie’:’id=123’ cookie结构：cookie是键值对的形式，可以设置多个 cookie流程：cookie是服务器返回的时候设置到浏览器的，保存在浏览器里面的数据，浏览器下一次同域的请求中会带上cookie 多个cookie：使用数组 1['id=123','name=donscoco'] 来表示 cookie属性： 属性之间使用;分隔; max-age和expires设置过期时间；[‘id=123’,’name=donscoco;max-age=10’] max-age是多久过期，expires是到什么时候过期; Secure只有在https的时候发送; HttpOnly无法通过document.cookie访问（为了安全考虑，如CSRF攻击）[‘id=123’,’name=donscoco;HttpOnly’]; cookie共享：不同域名之间的cookie是不能共享的；如果想在一级域名下所有的二级域名都能读到cookie可以在访问一级域名test.com时设置[‘id=123’,’name=donscoco;domain=test.com’]那么a.test.com和b.test.com都可以拿到同样的cookie；ps：不能跨域名设置cookie；e.g：a.test.com 不能给 test.com 设置cookie Cookie和SessionCookie不等于session；只是两者也是不同的概念，只是一般我们使用cookie来保存session，例如对于每个网站来说每个用户的session都是不一样的，这样客户端请求服务的时候会带上cookie，如果cookie中有之前存放的session，那么服务器就可以识别用户；这里session就是用于定位用户，只要能定位到用户，那就是一种session的实现方案，并不一定非要通过cookie来实现； HTTP连接 http的请求是建立在TCP的连接上的http请求发送的时候要先去创建一个tcp连接，在tcp连接上面把请求发送并且接受返回；在经过一次请求与返回之后，TCP和服务器的连接如果保持，那下次请求的时候可以直接使用这个TCP连接，但是在没有请求的时候是一种消耗，如果断开，下次请求的时候就要重新去进行三次握手建立连接；tcp默认会同时创建6个连接；如果连接满了剩余的请求就会进入等待，这里是说同时存在6个连接，传输后不一定会继续使用原来的连接，看connection会不会复用，可能是其他连接，但是同时存在的连接不超过6个 TCP长连接什么是长连接？客户端和服务端建立TCP连接后保持连接长连接的设置：&#39;connection&#39;:&#39;keep-alive&#39;客户端和服务端都可以设置这个字段，这里如果服务端返回的如果是不想保持，浏览器也还是会关掉浏览器请求会尽量地去复用已有的TCP连接，等有剩余的连接空出来;观察下图 ConnectionID和Waterfall字段； TCP短连接什么是短连接？客户端和服务端建立TCP连接经过一次请求返回后就关闭连接短连接的设置：’connection’:’close’浏览器不会去复用TCP连接，每个请求都建立一个新的TCP连接;观察下图 ConnectionID和Waterfall字段； 信道复用在http2里面；在TCP连接上面，我们可以并发地去放HTTP请求，也就是说，我们在连接一个网站(同域)的时候，只需要一个连接 数据协商 概念：客户端向服务端发送请求时，向服务端表明，自己希望拿到的数据格式是怎么样的。 请求 Accept 告诉服务器我希望的数据类型，可以是多个，用,分隔 Accept-Encoding 告诉服务器我的数据是怎样的编码方式进行传输,流行的有（gzip,deflate,br） Accept-Language 表示我希望要的语言类型,这个一般 是浏览器自动加的，浏览器会根据系统语言来填写这个 User-Agent 客户端的类型，告诉服务器我是PC端还是移动端 返回 Content-Type 表示我实际返回的是什么样的数据格式 Content-Encoding 声明我服务端返回的数据是什么编码 Content-Language 表示返回的语言类型 重定向服务端示例 客户端访问http://localhost:8080后跳转到http://localhost:8080/new展示这里要状态吗和Location一起使用，状态码设置200依然不会跳转 301和302区别301指永久跳转，302是临时跳转浏览器下次再遇到请求那个会跳转301的路径时，会自动再加上跳转后的路径，不用再去经过服务器处理301重定向告诉浏览器下一次再出现这个路径的访问的时候，直接在浏览器这边把他变成一个新的路径就可以了，不用经过服务器再去指定一个新的location而且还会默认在浏览器留下缓存 HTTPS HTTP概念：超文本传输协议（HTTP）是用于传输诸如HTML的超媒体文档的应用层协议。它被设计用于Web浏览器和Web服务器之间的通信，但它也可以用于其他目的。 HTTP遵循经典的客户端-服务端模型，客户端打开一个连接以发出请求，然后等待它收到服务器端响应。 HTTP是无状态协议，意味着服务器不会在两个请求之间保留任何数据（状态）。虽然通常基于TCP / IP层，但可以在任何可靠的传输层上使用;也就是说，一个不会静默丢失消息的协议，如UDP。HTTPS概念：HTTP Strict Transport Security (通常简称为HSTS) 是一个安全功能，它告诉浏览器只能通过HTTPS访问当前资源, 禁止HTTP方式.Strict Transport Security解决了这个问题；只要你通过HTTPS请求访问银行网站，并且银行网站配置好Strict Transport Security，你的浏览器知道自动使用HTTPS请求，这可以阻止黑客的中间人攻击的把戏。HTTP的包是明文传输HTTPS是在HTTP基础上进行加密 TTPS协议需要到CA申请证书，一般免费证书很少，需要交费。 HTTPS运行在SSL/TLS之上，SSL/TLS运行在TCP之上，所有传输的内容都经过加密的。 HTTP和HTTPS使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。加密公钥私钥主要是在握手的时候进行传输 私钥 公钥更多参考 http://www.wxtlife.com/2016/03/27/%E8%AF%A6%E8%A7%A3https%E6%98%AF%E5%A6%82%E4%BD%95%E7%A1%AE%E4%BF%9D%E5%AE%89%E5%85%A8%E7%9A%84%EF%BC%9F/ IN ACTION证书生成1openssl req -x509 -newkey rsa:2048 -nodes -sha256 -keyout localhost-privkey.pem -out localhost-cert.pem 得到 localhost-privkey.pem 和 localhost-cert.pem 文件 Nginx 配置:12345678910111213141516171819202122232425262728293031proxy_cache_path cache levels=1:2 keys_zone=my_cache:10m;server &#123; listen 80; # listen [::]:80 default_server; server_name test.com; # return 302 https://$server_name$request_uri; location / &#123; proxy_cache my_cache; proxy_pass http://127.0.0.1:8888; proxy_set_header Host $host; &#125;&#125;server &#123; listen 443 http2; server_name test.com; http2_push_preload on; ssl on; ssl_certificate_key ../certs/localhost-privkey.pem; ssl_certificate ../certs/localhost-cert.pem; location / &#123; proxy_cache my_cache; proxy_pass http://127.0.0.1:8888; proxy_set_header Host $host; &#125;&#125; HTTP2 数据的传输： http2 所有数据都是以二进制进行传输的，都是以帧进行传输 http1 里面传输是通过字符串来传输的 HTTP1.x的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑HTTP2.0的协议解析决定采用二进制格式，实现方便且健壮 多路复用 即连接共享，即每一个request都是是用作连接共享机制的。 一个request对应一个id，这样一个连接上可以有多个request，每个连接的request可以随机的混杂在一起，接收方可以根据request的 id将request再归属到各自不同的服务端请求里面。 多路复用原理图： 请求返回无序： http2 同一个连接发送多个请求不需要按照顺序来返回 这个也是数据用帧来传输带来的好处，也进一步带来了可以并发传输，大大增加了效率 头信息压缩 头信息压缩提高效率的功能； 在http1里面每次请求和返回的头都是要完整返回的，但是头里面很多字段都是字符串，会导致传输的额外开销 推送 传统的http请求只能客户端发起请求，然后服务器响应请求；客户端是主动方，服务端是被动方； 在HTTP2里面，服务端是可以主动发起数据传输的 举个栗子：html中都有一些css，js之类的文件，在请求了html文本之后，再根据css，js的地址去请求；顺序是先得到文本在浏览器解析后再去请求css和js，这是一个串行的过程；有推送功能后，在返回html时一起推送css和js，大大提高了效率 http2定义上没有和https相关，但是因为http2是google之前开发的一个叫spdy演化而来，所以，要使用http2要支持https HTTP和HTTPS区别 HTTPS协议需要到CA申请证书，一般免费证书很少，需要交费。 HTTP协议运行在TCP之上，所有传输的内容都是明文，HTTPS运行在SSL/TLS之上，SSL/TLS运行在TCP之上，所有传输的内容都经过加密的。 HTTP和HTTPS使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 HTTPS可以有效的防止运营商劫持，解决了防劫持的一个大问题。]]></content>
      <categories>
        <category>Protocol</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
        <tag>HTTPS</tag>
        <tag>HTTP2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[thread pools]]></title>
    <url>%2F2018%2F10%2F18%2FthreadPool%2F</url>
    <content type="text"><![CDATA[A simple example 123456789private Executor executor = new ThreadPoolExecutor(4, 10, 30, TimeUnit.SECONDS, new ArrayBlockingQueue(500));...executor.execute(new Runnable() &#123; @Override public void run() &#123; function(...); &#125;&#125;);.. Benefits of using thread pools Reduce resource consumption —Reduce the consumption of thread creation and destruction by reusing the created threads. Improve response speed Improving thread manageability —Monitoring and tuning Use of thread pool Creation of thread pool we can create a thread pool such like: 1new ThreadPoolExecutor(corePoolSize, maximumPoolSize,keepAliveTime, milliseconds,runnableTaskQueue, threadFactory,handler); parameter: corePoolSize : When a task is submitted to the thread pool, the thread pool creates a thread to execute the task, even if other idle base threads are able to execute the new tasks. The thread is not created until the number of tasks to be executed is greater than the thread pool’s basic size. maximumPoolSize : The maximum number of threads allowed by thread pool.If the queue is full and the number of threads created is less than the maximum number of threads, the thread pool creates new threads to perform tasks. keepAliveTime : The thread pool’s working threads are free after the idle thread. So if there are many tasks, and each task is executed in a relatively short time, you can adjust this time to improve the utilization of threads. milliseconds : runnableTaskQueue ： Block queue for saving tasks waiting to be excused. It can be as follows: ArrayBlockingQueue LinkedBlockingQueue SynchronousQueue — A blocking queue that does not store elements. Each insertion operation must wait until another thread calls the removal operation. PriorityBlockingQueue — An infinite blocking queue with priority RejectedExecutionHandler : When the queue and thread pool are full, indicating that the thread pool is saturated, a strategy must be adopted to handle the new tasks submitted. It can be as follows: AbortPolicy : throw Exception CallerRunsPolicy : use only the thread of the caller to run the task. DiscardOldestPolicy : discard the last task in the queue and perform the current task. DiscardPolicy : discard Submission of the task We can use execute to submit a task, but the execute method does not return a value, so it is impossible to determine whether the task was successfully executed by the thread pool. The following code shows that the task entered by the execute method is an instance of a Runnable class. 123456threadsPool.execute(new Runnable() &#123; @Override public void run() &#123; // TODO Auto-generated method stub &#125;&#125;); We can also submit a task using the submit method, which returns a future. We can use the future to determine whether the task is successful, get the return value through the future’s get method, block the get method until the task is complete, and block one using the get (long timeout, TimeUnit unit) method. Return immediately after a period of time, and it is possible that the task is not executed. 12345678910Future&lt;Object&gt; future = executor.submit(harReturnValuetask);try &#123; Object s = future.get();&#125; catch (InterruptedException e) &#123; // handler interruptedException&#125; catch (ExecutionException e) &#123; // handler executionexception&#125; finally &#123; executor.shutdown();&#125; Closure of thread pool We can close the thread pool by calling the shutdown or shutdownNow methods of the thread pool, which traverse the worker threads in the thread pool and then interrupt the thread one by one, so tasks that cannot respond to interruptions may never be terminated. But there are some differences. shutdownNow first sets the state of the thread pool to STOP, then tries to stop all threads executing or suspending tasks, and returns a list of tasks waiting to be executed, whereas shutdownNow just sets the state of the thread pool to SHUTDOWN, and then interrupts all threads that are not executing. Service threads. As long as one of the two closing methods is invoked, the isShutdown method returns true. Calling the isTerminated method returns true when all tasks are closed. As for which method we should invoke to close the thread pool, it should be determined by the task characteristics submitted to the thread pool, usually calling shutdown to close the thread pool, or calling shutdownNow if the task is not necessarily complete. Analysis of thread pool The main workflow of the thread pool is as follows:we can see from the above picture. When a new task is submitted to the thread pool, the processing flow of the thread pool is as follows: First, thread pool determines whether the basic thread pool is full. If it is not full, create a worker thread to perform the task. Full, then enter the next process. Secondly, the thread pool determines whether the work queue is full. If not, the new task will be stored in the work queue. Full, then enter the next process. Finally, the thread pool determines whether the entire thread pool is full. If it is not full, a new worker thread is created to perform the task, and when it is full, the saturation policy is given to handle the task. Source code 1234567891011public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); if (poolSize &gt;= corePoolSize || !addIfUnderCorePoolSize(command)) &#123; if (runState == RUNNING &amp;&amp; workQueue.offer(command)) &#123; if (runState != RUNNING || poolSize == 0) ensureQueuedTaskHandled(command); &#125;else if (!addIfUnderMaximumPoolSize(command)) reject(command); &#125;&#125; Working thread When a thread pool creates a thread, it encapsulates the thread as a worker thread, which, after executing the task, loops indefinitely to fetch the task in the work queue for execution. We can see this from the run method of Worker: 123456789101112public void run() &#123; try &#123; Runnable task = firstTask; firstTask = null; while (task != null || (task = getTask()) != null) &#123; runTask(task); task = null; &#125; &#125; finally &#123; workerDone(this); &#125;&#125; Rational configuration of thread pool To configure thread pools reasonably, we must first analyze the characteristics of tasks, which can be analyzed from the following aspects: Nature of task : CPU intensive tasks, IO intensive tasks and hybrid tasks. Tasks with different tasks can be processed separately from different sizes of thread pools. CPU intensive tasks are configured as small as possible, such as thread pools that configure Ncpu+1 threads. IO-intensive tasks configure as many threads as possible, such as 2 * Ncpu, because threads are not always executing tasks. Hybrid tasks, if they can be split, are split into a CPU-intensive task and an IO-intensive task. As long as the time difference between the two tasks is not too large, the throughput after decomposition is higher than that of serial execution. If the time difference between the two tasks is too large, it is not necessary. Decompose. We can get the CPU number of the current device through the Runtime.getRuntime ().AvailableProcessors () method. Priority of tasks : Tasks with different priorities can be handled using priority queue PriorityBlockingQueue. It allows high-priority tasks to be executed first, and it is important to note that low-priority tasks may never be executed if high-priority tasks are always submitted to the queue. Task execution time : Tasks with different execution times can be handled by thread pools of different sizes, or priority queues can be used to allow tasks with short execution times to execute first. Task dependency : Is it dependent on other system resources, such as database connection? Depending on the database connection pool task, because threads need to wait for the database to return the results after submitting SQL, if the longer the wait, the longer the CPU idle time, then the number of threads should be set, so as to better use the CPU. It is suggested that bounded queues be used. Bounded queues can increase the stability and early warning capability of the system, and can be set up as large as needed, such as several thousand.It is suggested that bounded queues be used. Bounded queues can increase the stability and early warning capability of the system, and can be set up as large as needed, such as several thousand. If unbounded queues are used, if the system goes wrong, memory may be crammed, affecting other systems. Thread pool monitoring Monitor the parameters provided by thread pool. There are properties in the thread pool that can be used when monitoring thread pools. taskCount The number of tasks that thread pools need to perform. completedTaskCount The number of tasks that the thread pool has completed in the process of running. Less than or equal to taskCount. largestPoolSize The maximum number of threads that thread pool has ever created. This data can be used to see if the line pool is full. getPoolSize The number of threads in the thread pool.-getActiveCount Gets the number of threads of activity. Monitor by extending thread pool. By inheriting the thread pool and overwriting the beforeExecute, afterExecute, and terminated methods of the thread pool, we can do something before, after, and before the thread pool is closed. For example, the average execution time, maximum execution time and minimum execution time of monitoring tasks. These methods are empty methods in the online pool. Such as:1protected void beforeExecute(Thread t, Runnable r) &#123; &#125; More Transmission gate : ifeveTransmission gate : a tiny huskyTransmission gate : jonnyTransmission gate : OracleTransmission gate : csdnTransmission gate : csdn]]></content>
      <categories>
        <category>Thread</category>
      </categories>
      <tags>
        <tag>Thread</tag>
        <tag>Util</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2F2018%2F01%2F30%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[前言这里只是在学习linux相关知识过程中，对常用命令做的一个总结笔记更多命令可访问 http://man.linuxde.net/ 命令格式命令 [选项] [参数]个别命令可以不遵循这种格式 ls [选项] [目标文件或目录]查询目录中的内容 ls -als -l 显示文件详细信息ls -i 显示文件的inode系统每个文件或文件夹都有他自己的inode号，查找根据inode号查 cd 切换目录pwd 显示当前路径 Ctrl cctrl ltab 补全 linux中，对于一些守护进程的命令，服务命名上一般会加d 如 httpd atd 服务 环境相关语言LANG=en_USLANG=zh_CH.utf8 时间#文件与目录处理命令 建立目录mkdir -p [目录名] 建立目录-p 递归创建 切换目录cd [目录]cd ~cd ..cd .cd - 相对路径cd ./dons 进入当前目录下的dons目录cd ../home/dons进入上一级目录下的home的dons 绝对路径cd /home/dons 查询目录所在位置pwd –print working directory 删除空目录rmdir [目录名]remove empty directory 删除文件或目录rm -rf [文件或目录]-r 删除目录-f 强制删除小心删除 rm -rf /删除掉以/开头的，rm -rf 可以让系统“自杀” 复制命令cp [选项] [原文件或目录] [目标目录]如果目标目录不加文件名，则原名复制；eg：cp test /tmp/反之，则是等名复制；eg：cp -a test /tmp/new_test-r 复制目录,不加-r 默认复制文件-p 带文件属性复制，不加复制的时间是复制的时候的文件-d 若源文件是链接文件，则复制链接属性-a 相当于上面 -pdr 剪切或改名命令mv [源文件或目录] [目标目录] linux 链接命令ln -s [原文件] [目标文件]link-s 创建软连接 硬链接拥有相同的i 节点和存储block快，可以看做是同一个文件可通过i节点识别不能跨分区不能针对目录使用就算x没了，硬链接也还是能指到id，相当于一个文件，可以使用ls -i可以发现两个id都是一样的图中 的 2 表示 这个文件有多少个链接数 软链接类似windows的快捷方式软链接拥有自己的block块，但数据块中只保存原文件的文件名和i节点号，没有实际数据例如 lrwxrwxrwx 中的l 就是指链接文件修改任意文件，另一个都改变删除原文件，软链接不能使用 搜索命令文件搜索命令locate [文件名]在后台数据库中按文件名搜索，搜索比find快；后台数据库—/var/lib/mlocate 有的版本可能叫其他的例如 slocate locatedb新建的文件没有记录进入后台数据库，因为后台数据库不是实时更新，也可以立刻更新 updatedb缺点：只能搜索文件名 更多：文件搜索规则 /etc/updatedb.conf 配置文件 find [搜索范围] [搜索条件]（慢）遍历整个根-name find /home -name ‘dons’-iname 不区分大小写-user 按所有者搜索-nouser 搜索没有所有者的文件，一般用来清理垃圾文件-mtime +10 搜索十天前修改的文件-atime +10 搜索十天前访问的文件-ctime +10 搜索十天前修改属性的文件这里+10 10 -10 分别是 今天1月20号 1/10号之前的，1/10当天，1/10号-1/20 -size [-25k|25k|+25k] 查找文件大小是小于25k的 等于25k的 大于25k的-size +20k -a -50k 查找文件大小是大于20k并且小于50k的文件 -a:and -o:or-inum 558528 查找 inode 节点号为 558528 的文件 linux 中的通配符*匹配任意内容?匹配任意字符[]匹配任意中括号内的字符更多详见https://abcfy2.gitbooks.io/linux_basic/content/first_sense_for_linux/command_learning/wildcard.html 命令搜索命令whereis-b:只查找可执行命令-m:只查找帮助文档which和whereis 一样，多加了别名 更多 whoami whatis whereis 字符串搜索命令grep [选项] 字符串 文件名在文件中匹配符合条件的字符串-i 忽略大小写-v 排除指定字符串如果想要使用模糊去匹配需要使用正则表达式，find中使用通配符 更多：管道 cat test.log | grep “2018-1-30” 帮助命令man [命令]manul 压缩与解压缩常用压缩格式 .zip .gz .bz2 .tar.gz .tar.bz2zip 压缩文件名 原文件 – 压缩文件zip -r 压缩文件名 原目录 – 压缩目录unzip 解压缩文件 gzip 原文件 – 压缩为.gz 格式；原文件会消失gzip -c 原文件 &gt; 压缩文件 – 压缩为.gz 个格式；原文件保留gzip -r 原文件gzip -d 压缩文件 – 解压缩文件gunzip 压缩文件 – 解压缩文件 bzip2 原文件 –压缩为.bz2格式，不保留原文件bzip2 -k 原文件 –压缩后保留原文件bzip2 -d 压缩文件 – 解压缩文件bunzip2 压缩文件 – 解压缩 -k 保留压缩文件 .tar.gz .tar.bz2 这类文件是因为gz，bz2打包不能连着文件夹一起打包，所以先打包成了.tar 再将这个文件打包成.gz打包命令 tartar -cvf 打包文件名 源文件 – 打包成.tar-c 打包-v 显示过程-f 指定打包后的文件-x 解打包tar -xvf 打包文件-zcvf 直接压缩为 .tar.gz-zxvf 直接解压缩 .tar.gz-jcvf 直接压缩为 .tar.bz2-jxvf 直接解压缩 .tar.bz2 linxu常见目录作用 / 根目录/bin 命令保存目录（普通用户权限）/sbin 命令保存目录（root权限）/boot 启动目录，包含启动相关文件，和开机有关/dev 设备文件保存目录/etc 配置文件保存目录/home 普通用户家目录/lib 系统库保存目录/mnt 系统挂载目录/media 挂载目录（常用于光盘挂载）/root 超级用户家目录/tmp 临时目录/proc 直接写入内存的/sys 直接写入内存的/usr 系统软件资源目录/var 系统相关文档内容 linux 关机与重启关机与重启shutdowm [选项] 时间-c: 取消前一个关机命令-h: 关机-r: 重启 时间的格式：05:30 凌晨5点30分 时间可以 now执行之后无法执行其他命令，但是可以将任务放到后台：1shutdown -r 05:30 &amp; 更多重启：reboot,init 6更多关机：不推荐使用，不会保存当前状态等信息 halt poweroff init 0为什么 init 0 init 6 ？这有关于linux系统运行级别https://www.ibm.com/developerworks/cn/linux/l-lpic1-v3-101-3/index.html 退出登录logout 挂载命令什么是挂载？在linux中所有的存储设备，都必须挂载后才能正常使用，windows中用字母作为系统盘符，linux中用目录作为盘符；所以挂载其实就是分配盘符，只是分配的盘符叫挂载点，分配的过程叫挂载硬盘的分区是第一次设置好挂载后，以后每次启动机器自动挂载的而光盘，U盘等存储设备必须人为地来挂载 查询与自动挂载mount 查询系统中已挂载的设备mount -a 依据配置文件/etc/fstab 的内容，自动挂载 12345/dev/vda1 on / type ext3 (rw,noatime,acl,user_xattr)proc on /proc type proc (rw)sysfs on /sys type sysfs (rw)devpts on /dev/pts type devpts (rw,mode=0620,gid=5)none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw) /dev/vda1 on / type ext3 (rw,noatime,acl,user_xattr)中dev是硬件设备目录vd 是接口 还有 sd等a 代表硬盘5 是分区on 是挂载到/ 挂载到根分区ext3 是文件系统(rw,noatime,acl,user_xattr)是权限 proc on /proc type proc (rw)和sysfs on /sys type sysfs (rw)中proc和sysfs是内存的挂载点，内核自己挂载的，不要乱动！！！ 自动挂载配置/etc/fstab 文件是系统自动挂载配置 挂载命令格式mount [-t 文件系统] [-o 特殊选项] 设备文件名 挂载点-t 文件系统： 加入文件系统类型来制指定挂载类型，可以是ext3，ext4，iso9660等文件系统-o 特殊选项： 可以指定挂载的额外选项eg：mount -o remount,noexec /home/ 这里的noexec会使得挂载为不能执行，坑人专用 执行完记得改回来！！！mount -o remount,exec /home/这里没有写 设备文件名 /dev/vda5 是因为 自动挂载配置文件里面已经能自动找到对应的 名字了对于没有在/etc/fstab 文件中的，例如 U盘，光盘等就要写上设备文件路径 挂载光盘 建立挂载点 mkdir /mnt/cdrom/ 挂载光盘 mount -t iso9660 /dev/cdrom /mnt/cdrom 或 mount /dev/sr0 /mnt/cdrom 省略文件系统选项，因为系统默认知道光盘的文件系统是 iso9660因为挂载默认会去执行读写权限，而光盘是不能写的，所以挂载光盘时候会出现mount: block device /dev/sr0 is write-protected, mounting read-only 这是正常的。 卸载命令umount 设备名或者挂载点umount /dev/sr0umount /mnt/cdrom 挂载U盘fdisk -l 查看U盘设备文件名;一般是 /dev/sdb1mount -t vfat /dev/sdb1 /mnt/usb/linux 默认是不支持NTFS文件系统的;想使用需要去往内核装NTFS驱动；或者安装fts-3G linux用户登陆查看命令w12319:10:04 up 8:17, 1 user, load average: 0.00, 0.00, 0.00USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 58.248.229.155 19:10 0.00s 0.00s 0.00s w 19:10:04 up 8:17, 1 user, load average: 0.00, 0.00, 0.00中19:10:04 当前系统时间up 8:17 系统到现在运行了多少个小时1 user 总登陆了1个用户load average: 0.00, 0.00, 0.00 系统在 1分钟 5分钟 15分钟的 平均负载pts/0 登陆方式是远程58.248.229.155 从哪个ip过来的USER：登陆的用户名TTY：登陆终端FROM：从哪个IP地址登陆LOGIN：登陆时间IDLE：用户闲置时间JCPU：指的是和该终端连接的所有进程占用的时间。这个时间里并不包括过去的后台作业时间，但包含当前正在运行的后台作业占用的时间PCPU：当前进程所占用的时间WHAT：当前正在运行的命令 last可以查看系统中所有的用户登陆信息和系统重启信息这个可以发现有黑客进入了你的计算机last命令默认读取的是 /var/log/wtmp 文件数据lastlog 记录的是系统所有用户的最后一次登陆lastlog 命令默认读取的是 /var/log/lastlog 文件 用户管理切换用户su [用户名]之后输入相应密码即可 添加用户useradduseradd 用户名passwd 目标用户名 –给指定用户名设置密码 useradd -G donscoco_group donscoco2 –添加用户的时候指定用户分组，没有指定分组系统会默认添加一个和用户名一样的分组 修改用户账户usermod –l donscoco dons –将用户的用户名dons改为donscocousermod –g dons_group donscoco –将用户 donscoco 加入到 dons_group组中 删除用户账户userdel donscoco –删除用户 donscocouserdel –r donscoco –删除用户，同时删除他的工作目录 查看用户id donscoco –查找donscoco的信息 添加用户组groupadd –g 666 donsgroup 添加一个groupID为666的用户组donsgroup 修改用户组groupmod –n dons_group donsgroup 修改用户组donsgroup为dons_group 删除用户组groupdel dons_group 为用户组添减成员gpasswd -a donscoco dons_group 为dons_group 组添加 donscocogpasswd -d donscoco dons_group 为dons_group 组移去 donscoco 文本处理查看文件内容查看文件内容有多个 cat,head,tail,more,vi catcat [option] [param] -b或–number-nonblank：和-n相似，只不过对于空白行不编号；-s或–squeeze-blank：当遇到有连续两行以上的空白行，就代换为一行的空白行；-A：显示不可打印字符，行尾显示“$”；-e：等价于”-vE”选项；-t：等价于”-vT”选项； headhead [option] [param] option:-n&lt;数字&gt;：指定显示头部内容的行数；-c&lt;字符数&gt;：指定显示头部内容的字符数；-v：总是显示文件名的头信息；-q：不显示文件名的头信息。 tailtail [option] [param] option:-n&lt;数字&gt;：指定显示头部内容的行数；-c&lt;字符数&gt;：指定显示头部内容的字符数；-v：总是显示文件名的头信息；-q：不显示文件名的头信息。 visual interfacevi 的升级版本 vim ，主要对。。。 vi的操作模式 command mode: 命令模式 Insert mode: 输入模式 Last line: 底行模式 模式的切换：输入模式 -&gt; 命令模式 Esc 键命令模式 -&gt; 底行模式 : 键 /键底行模式 -&gt; 命令模式 Esc 键 例子vim + file_name 打开文件并把光标定位到最后一行vim +3 file_name 打开文件并把光标定位到第3行vim +/donscoco file_name 打开文件并把光标定位到donscoco第一次出现的那一行，这时候按字母n可以往下定位（命令模式）vim file_name_1 file_name_2 file_name_3 一次性编辑或创建多个文件，在底行模式中输入n可以切换文件 命令模式常用命令： o: 在光标下一行插入一行并切入到输入模式 dd:删除光标所在一整行 yy:复制光标所在行 p:在光标所在行下方粘贴 P：在光标所在行上方粘贴 底行模式常用命令： :w 执行保存修改 :q 退出 :! 强制执行 :15 快速定位到第15行；直接接一个数字快速定位到对应行 /donscoco 从当前光标开始往下搜索donscoco ?donscoco 从当前光标开始往上搜索donscoco :ls 列出打开的所有文件 :n 切换到下一个文件 :N 切换到上一个文件 磁盘管理磁盘管理基本命令查看磁盘分区使用状况df 查看磁盘分区使用状况 l 仅显示本地磁盘 （默认） a 显示所有文件系统的磁盘使用情况 h 以1024进制计算最合适的单位显示磁盘容量 H 以1000进制计算最合适的单位显示磁盘容量 (工业上的计算，这就是为什么8G的U盘不到8G的) T 显示磁盘分区类型 t 跟参数，查出指定对应文件类型的所有分区 123[root@VM_0_8_centos vim_learning_dir]# dfFilesystem 1K-blocks Used Available Use% Mounted on/dev/vda1 51605116 2709772 46273980 6% / 1234567[root@VM_0_8_centos vim_learning_dir]# df -aFilesystem 1K-blocks Used Available Use% Mounted on/dev/vda1 51605116 2709780 46273972 6% /proc 0 0 0 - /procsysfs 0 0 0 - /sysdevpts 0 0 0 - /dev/ptsnone 0 0 0 - /proc/sys/fs/binfmt_misc 12345678910[root@VM_0_8_centos vim_learning_dir]# df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 50G 2.6G 45G 6% /[root@VM_0_8_centos vim_learning_dir]# df -ahFilesystem Size Used Avail Use% Mounted on/dev/vda1 50G 2.6G 45G 6% /proc 0 0 0 - /procsysfs 0 0 0 - /sysdevpts 0 0 0 - /dev/ptsnone 0 0 0 - /proc/sys/fs/binfmt_misc 12345678910[root@VM_0_8_centos vim_learning_dir]# df -TFilesystem Type 1K-blocks Used Available Use% Mounted on/dev/vda1 ext3 51605116 2709840 46273912 6% /[root@VM_0_8_centos vim_learning_dir]# df -aTFilesystem Type 1K-blocks Used Available Use% Mounted on/dev/vda1 ext3 51605116 2709840 46273912 6% /proc proc 0 0 0 - /procsysfs sysfs 0 0 0 - /sysdevpts devpts 0 0 0 - /dev/ptsnone binfmt_misc 0 0 0 - /proc/sys/fs/binfmt_misc 123[root@VM_0_8_centos vim_learning_dir]# df -lahT -t ext3Filesystem Type Size Used Avail Use% Mounted on/dev/vda1 ext3 50G 2.6G 45G 6% / 统计磁盘上的文件大小du 用于统计磁盘上的文件大小 b 以byte单位统计 k 以KB为单位统计 m 以MB为单位统计 h 按照1024进制以最合适的单位统计 H 按照1000进制以最合适的单位统计 s 指定统计目标 12[root@VM_0_8_centos ~]# du -hs *.rpm132M jdk-7u80-linux-x64.rpm 硬盘分区和格式化主分区和扩展分区总数不能超过4个；扩展分区只能有 0-1 个；扩展分区不能直接存取数据，必须建立逻辑分区当硬盘空间消耗殆尽怎么办？在保留原有硬盘的基础上，给服务器添加新的硬盘 添加新硬盘–如果硬件的主板支持热插拔，就可以不关机添加新硬盘 linux中的MBR分区linux中硬件设备都是以文件的形式存在于根目录下的dev目录下硬件设备都是linux自动识别的；但是不能立即使用；必须对硬盘进行分区，格式化，挂载后才能使用 MBR模式的主分区 不超过4个，单个分区容量最大2TB fdiskUsage: fdisk [options] change partition table fdisk [options] -l list partition table(s) fdisk -s give partition size(s) in blocksOptions: -b sector size (512, 1024, 2048 or 4096) -c switch off DOS-compatible mode -h print help -u give sizes in sectors instead of cylinders -v print version -C specify the number of cylinders -H specify the number of heads -S specify the number of sectors per track 1234567891011[root@VM_0_8_centos ~]# fdisk -lDisk /dev/vda: 53.7 GB, 53687091200 bytes255 heads, 63 sectors/track, 6527 cylindersUnits = cylinders of 16065 * 512 = 8225280 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x00081267 Device Boot Start End Blocks Id System/dev/vda1 * 1 6528 52427776 83 Linux 如果有为进行分区的磁盘会显示Disk /dev/sdb doesn&#39;t contain a valid partition table 通过 fdisk [设备路径] 可以进入分区模式 123456789101112131415161718Command (m for help): mCommand action a toggle a bootable flag b edit bsd disklabel c toggle the dos compatibility flag d delete a partition l list known partition types m print this menu n add a new partition o create a new empty DOS partition table p print the partition table q quit without saving changes s create a new empty Sun disklabel t change a partition's system id u change display/entry units v verify the partition table w write table to disk and exit x extra functionality (experts only) 进行分区1234Command (m for help): nCommand action e extended p primary partition (1-4) 指定 主分区或者 扩展分区 ，这里举例指定为p，之后会问分区的编号1Partition number (1-4): 之后询问开始分区的位置12First cylinder (1-104023, default 1): Using default value 1 不输入默认就是第一个可用的位置询问分区最后一个位置，可用输入 +[值][单位] 来表示从开始后多少，系统自己换算，默认的话就是全部都用完1Last cylinder, +cylinders or +size&#123;K,M,G&#125; (1-104023, default 104023): +3000M 然后查看分区1234567891011Command (m for help): pDisk /dev/vda1: 53.7 GB, 53686042624 bytes16 heads, 63 sectors/track, 104023 cylindersUnits = cylinders of 1008 * 512 = 516096 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x98f7960e Device Boot Start End Blocks Id System/dev/vda1p1 1 6096 3072352+ 83 Linux 这里添加的分区并非写入磁盘，只是一个方案 删除分区方案12345678910111213Command (m for help): dPartition number (1-5): 2Command (m for help): pDisk /dev/vda1: 53.7 GB, 53686042624 bytes16 heads, 63 sectors/track, 104023 cylindersUnits = cylinders of 1008 * 512 = 516096 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x531b8bcc Device Boot Start End Blocks Id System 这里扩展分区删除，对应的逻辑分区也会删除 分区方案写入磁盘分区表1Command (m for help): w 完整例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364Command (m for help): pDisk vda1: 42.9 GB, 42947575808 bytes16 heads, 63 sectors/track, 83216 cylindersUnits = cylinders of 1008 * 512 = 516096 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0xb5b6912eDevice Boot Start End Blocks Id SystemCommand (m for help): nCommand action e extended p primary partition (1-4)pPartition number (1-4): 1First cylinder (1-83216, default 1): 1Last cylinder, +cylinders or +size&#123;K,M,G&#125; (1-83216, default 83216): 10240Command (m for help): pDisk vda1: 42.9 GB, 42947575808 bytes16 heads, 63 sectors/track, 83216 cylindersUnits = cylinders of 1008 * 512 = 516096 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0xb5b6912eDevice Boot Start End Blocks Id Systemvda1p1 1 10240 5160928+ 83 LinuxCommand (m for help): nCommand action e extended p primary partition (1-4)pPartition number (1-4): 2First cylinder (10241-83216, default 10241): 10241Last cylinder, +cylinders or +size&#123;K,M,G&#125; (10241-83216, default 83216): 20480Command (m for help): pDisk vda1: 42.9 GB, 42947575808 bytes16 heads, 63 sectors/track, 83216 cylindersUnits = cylinders of 1008 * 512 = 516096 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0xb5b6912eDevice Boot Start End Blocks Id Systemvda1p1 1 10240 5160928+ 83 Linuxvda1p2 10241 20480 5160960 83 LinuxCommand (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.WARNING: Re-reading the partition table failed with error 22: Invalid argument.The kernel still uses the old table. The new table will be used atthe next reboot or after you run partprobe(8) or kpartx(8)Syncing disks.[root@iZzf6hdnuqzu7wZ dev]# linux中的GPT分区GPT的主分区个数几乎没有限制 128个单个分区容量也几乎没有限制 1EB（EB,PB,TB,GB） parted12345[root@VM_0_8_centos ~]# partedGNU Parted 2.3Using /dev/sdaWelcome to GNU Parted! Type 'help' to view a list of commands.(parted) 一般进来默认会在 /dev/sda ，而要进行分区的新硬盘在别的地方，所以要先进行切换 123(parted) select /dev/sdbUsing /dev/sdb(parted) 给目标硬盘指定分区表的类型，然后才能给硬盘添加分区 1(parted) makelabel gpt 查看当前硬盘分区详情；查看所有是print all 1(parted) print 添加分区,开始和结束时间单位是M，和MBR默认的块不一样，使用后系统会警告数据块没有对齐，无法达到最佳性能 (查询4K对齐是什么？) 1234567(parted) mkpartPartition name? []? donscocoFile system type? [ext2]?Start? 0End? 2000Warning：The resulting partition is not properly aligned for best performanceIgnore/Cancel?cancel 取消掉后，牺牲1M来达到对齐 12345(parted) mkpartPartition name? []? donscocoFile system type? [ext2]?Start? 1End? 2000 这样添加就完成了也可以使用一行命令来添加1mkpart test 2000 3000 test 分区 (2000,3000] M 当建立的分区出现重叠的时候会出现警告，并且系统会给出建议的分区 删除分区rm [分区编号]1(parted) rm 3 退出分区quit 分区格式化mkfsmkfs -t [文件系统类型] [设备路径名称]这样就可以把设备格式化成对应的[文件系统类型] 挂载分区mount [设备] [挂载点]分区默认挂载目录是 /mnt 目录分区挂载的时候，必须挂载到一个已经存在的挂载点unmount [挂载点] mount 挂载的是临时的。下次开机又没了。要永久挂载需要到配置文件/etc/fstab中添加开机挂载如/dev/sda1 /mnt/donscoco ext3 default 0 0 swap交换分区如何为硬盘添加swap交换分区？ 建立一个普通的linux分区 修改分区类型的16进制编码 格式化交换分区 启用交换分区 1Command (m for help): t 选择想要作为swap的分区编号1Hex code (type L to list code):L 查看编码的列表12345678910111213141516171819202122232425262728Hex code (type L to list codes): L 0 Empty 24 NEC DOS 81 Minix / old Lin bf Solaris 1 FAT12 39 Plan 9 82 Linux swap / So c1 DRDOS/sec (FAT- 2 XENIX root 3c PartitionMagic 83 Linux c4 DRDOS/sec (FAT- 3 XENIX usr 40 Venix 80286 84 OS/2 hidden C: c6 DRDOS/sec (FAT- 4 FAT16 &lt;32M 41 PPC PReP Boot 85 Linux extended c7 Syrinx 5 Extended 42 SFS 86 NTFS volume set da Non-FS data 6 FAT16 4d QNX4.x 87 NTFS volume set db CP/M / CTOS / . 7 HPFS/NTFS 4e QNX4.x 2nd part 88 Linux plaintext de Dell Utility 8 AIX 4f QNX4.x 3rd part 8e Linux LVM df BootIt 9 AIX bootable 50 OnTrack DM 93 Amoeba e1 DOS access a OS/2 Boot Manag 51 OnTrack DM6 Aux 94 Amoeba BBT e3 DOS R/O b W95 FAT32 52 CP/M 9f BSD/OS e4 SpeedStor c W95 FAT32 (LBA) 53 OnTrack DM6 Aux a0 IBM Thinkpad hi eb BeOS fs e W95 FAT16 (LBA) 54 OnTrackDM6 a5 FreeBSD ee GPT f W95 Ext'd (LBA) 55 EZ-Drive a6 OpenBSD ef EFI (FAT-12/16/10 OPUS 56 Golden Bow a7 NeXTSTEP f0 Linux/PA-RISC b11 Hidden FAT12 5c Priam Edisk a8 Darwin UFS f1 SpeedStor 12 Compaq diagnost 61 SpeedStor a9 NetBSD f4 SpeedStor 14 Hidden FAT16 &lt;3 63 GNU HURD or Sys ab Darwin boot f2 DOS secondary 16 Hidden FAT16 64 Novell Netware af HFS / HFS+ fb VMware VMFS 17 Hidden HPFS/NTF 65 Novell Netware b7 BSDI fs fc VMware VMKCORE 18 AST SmartSleep 70 DiskSecure Mult b8 BSDI swap fd Linux raid auto1b Hidden W95 FAT3 75 PC/IX bb Boot Wizard hid fe LANstep 1c Hidden W95 FAT3 80 Old Minix be Solaris boot ff BBT 1e Hidden W95 FAT1Hex code (type L to list codes): 这里选择82；然后保存1Command (m for help): w 修改完要格式化交换分区swap1[root@VM_0_8_centos ~]# mkswap /dev/[选为swap的分区设备名] 然后启用swap1[root@VM_0_8_centos ~]# swapon /dev/[选为swap的分区设备名] 也可以停掉1[root@VM_0_8_centos ~]# swapoff /dev/[选为swap的分区设备名] 大杂烩查看文件的格式file [文件名] 查看ARP 缓存arp -a]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Thread and Process]]></title>
    <url>%2F2018%2F01%2F30%2FProcess%26%26Thread%2F</url>
    <content type="text"><![CDATA[Key difference: Thread and Process are two closely related terms in multi-threading. The main difference between the two terms is that the threads are a part of a process.i.e. a process may contain one or more threads, but a thread cannot contain a process. In programming, there are two basic units of execution: processes and threads. They both execute a series of instructions. Both are initiated by a program or the operating system. This article helps to differentiate between the two units. A process is an instance of a program that is being executed. It contains the program code and its current activity. Depending on the operating system, a process may be made up of multiple threads of execution that execute instructions concurrently. A program is a collection of instructions; a process is the actual execution of those instructions. A process has a self-contained execution environment. It has a complete set of private basic run-time resources; in particular, each process has its own memory space. Processes are often considered similar to other programs or applications. However, the running of a single application may in fact be a set of cooperating processes. To facilitate communication between the processes, most operating systems use Inter Process Communication (IPC) resources, such as pipes and sockets. The IPC resources can also be used for communication between processes on different systems. Most applications in a virtual machine run as a single process. However, it can create additional processes using a process builder object. In computers, a thread can execute even the smallest sequence of programmed instructions that can be managed independently by an operating system. The applications of threads and processes differ from one operating system to another. However, the threads are made of and exist within a process; every process has at least one. Multiple threads can also exist in a process and share resources, which helps in efficient communication between threads. On a single processor, multitasking takes place as the processor switches between different threads; it is known as multithreading. The switching happens so frequently that the threads or tasks are perceived to be running at the same time. Threads can truly be concurrent on a multiprocessor or multi-core system, with every processor or core executing the separate threads simultaneously. In summary, threads may be considered lightweight processes, as they contain simple sets of instructions and can run within a larger process. Computers can run multiple threads and processes at the same time. Comparison between Process and Thread:| | Process | Thread || —————- | —————————————- | —————————————- || Definition | An executing instance of a program is called a process. | A thread is a subset of the process. || Process | It has its own copy of the data segment of the parent process. | It has direct access to the data segment of its process. || Communication | Processes must use inter-process communication to communicate with sibling processes. | Threads can directly communicate with other threads of its process. || Overheads | Processes have considerable overhead. | Threads have almost no overhead. || Creation | New processes require duplication of the parent process. | New threads are easily created. || Control | Processes can only exercise control over child processes. | Threads can exercise considerable control over threads of the same process. || Changes | Any change in the parent process does not affect child processes. | Any change in the main thread may affect the behavior of the other threads of the process. || Memory | Run in separate memory spaces. | Run in shared memory spaces. || File descriptors | Most file descriptors are not shared. | It shares file descriptors. || File system | There is no sharing of file system context. | It shares file system context. || Signal | It does not share signal handling. | It shares signal handling. || Controlled by | Process is controlled by the operating system. | Threads are controlled by programmer in a program. || Dependence | Processes are independent. | Threads are dependent. | http://www.differencebetween.info/difference-between-process-and-thread]]></content>
      <categories>
        <category>Thread</category>
      </categories>
      <tags>
        <tag>process</tag>
        <tag>thread</tag>
        <tag>Key difference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iptables配置防火墙]]></title>
    <url>%2F2018%2F01%2F30%2Fiptables%E9%85%8D%E7%BD%AE%E9%98%B2%E7%81%AB%E5%A2%99%2F</url>
    <content type="text"><![CDATA[查看端口状态1netstat -tnl iptablesiptables [option] [parms] [option]: -t&lt;表&gt;：指定要操纵的表；-A：向规则链中添加条目；-D：从规则链中删除条目；-i：向规则链中插入条目；-R：替换规则链中的条目；-L：显示规则链中已有的条目；-F：清楚规则链中已有的条目；-Z：清空规则链中的数据包计算器和字节计数器；-N：创建新的用户自定义规则链；-P：定义规则链中的默认目标；-h：显示帮助信息；-p：指定要匹配的数据包协议类型；-s：指定要匹配的数据包源ip地址；-j&lt;目标&gt;：指定要跳转的目标；-i&lt;网络接口&gt;：指定数据包进入本机的网络接口；-o&lt;网络接口&gt;：指定数据包要离开本机所使用的网络接口。 iptables命令选项输入顺序：iptables -t 表名 &lt;-A/I/D/R&gt; 规则链名 [规则号] &lt;-i/o 网卡名&gt; -p 协议名 &lt;-s 源IP/源子网&gt; –sport 源端口 &lt;-d 目标IP/目标子网&gt; –dport 目标端口 -j 动作 表名：raw：高级功能，如：网址过滤。mangle：数据包修改（QOS），用于实现服务质量。net：地址转换，用于网关路由器。filter：包过滤，用于防火墙规则。 规则链名包括：INPUT链：处理输入数据包。OUTPUT链：处理输出数据包。PORWARD链：处理转发数据包。PREROUTING链：用于目标地址转换（DNAT）。POSTOUTING链：用于源地址转换（SNAT）。 动作包括：accept：接收数据包。DROP：丢弃数据包。REDIRECT：重定向、映射、透明代理。SNAT：源地址转换。DNAT：目标地址转换。MASQUERADE：IP伪装（NAT），用于ADSL。LOG：日志记录。 例子：1.清除已有iptables规则123iptables -Fiptables -Xiptables -Z 2.开放指定的端口123456789iptables -A INPUT -s 127.0.0.1 -d 127.0.0.1 -j ACCEPT #允许本地回环接口(即运行本机访问本机)iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT #允许已建立的或相关连的通行iptables -A OUTPUT -j ACCEPT #允许所有本机向外的访问iptables -A INPUT -p tcp --dport 22 -j ACCEPT #允许访问22端口iptables -A INPUT -p tcp --dport 80 -j ACCEPT #允许访问80端口iptables -A INPUT -p tcp --dport 21 -j ACCEPT #允许ftp服务的21端口iptables -A INPUT -p tcp --dport 20 -j ACCEPT #允许FTP服务的20端口iptables -A INPUT -j reject #禁止其他未允许的规则访问iptables -A FORWARD -j REJECT #禁止其他未允许的规则访问 3.屏蔽IP1234iptables -I INPUT -s 123.45.6.7 -j DROP #屏蔽单个IP的命令iptables -I INPUT -s 123.0.0.0/8 -j DROP #封整个段即从123.0.0.1到123.255.255.254的命令iptables -I INPUT -s 124.45.0.0/16 -j DROP #封IP段即从123.45.0.1到123.45.255.254的命令iptables -I INPUT -s 123.45.6.0/24 -j DROP #封IP段即从123.45.6.1到123.45.6.254的命令是 4.查看已添加的iptables规则123456789101112131415iptables -L -n -vChain INPUT (policy DROP 48106 packets, 2690K bytes) pkts bytes target prot opt in out source destination 5075 589K ACCEPT all -- lo * 0.0.0.0/0 0.0.0.0/0 191K 90M ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:221499K 133M ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:804364K 6351M ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED 6256 327K ACCEPT icmp -- * * 0.0.0.0/0 0.0.0.0/0 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 3382K packets, 1819M bytes) pkts bytes target prot opt in out source destination 5075 589K ACCEPT all -- * lo 0.0.0.0/0 0.0.0.0/0 5.删除已添加的iptables规则将所有iptables以序号标记显示，执行：1iptables -L -n --line-numbers 比如要删除INPUT里序号为8的规则，执行：1iptables -D INPUT 8 更多：]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>防火墙</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
</search>
